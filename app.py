import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import gspread
import os
import json
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from plotly.subplots import make_subplots

# Set page configuration with improved styling
st.set_page_config(
    page_title="B√°o c√°o ch·∫•t l∆∞·ª£ng CF MMB",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS to improve the look and feel
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        font-weight: 600;
        color: #1E3A8A;
        margin-top: 1rem;
        margin-bottom: 0.5rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .metric-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #64748b;
    }
    .metric-value {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E3A8A;
    }
    .stDataFrame {
        border-radius: 10px !important;
        overflow: hidden;
    }
    .stDataFrame table {
        border-collapse: collapse;
        width: 100%;
    }
    .stDataFrame th {
        background-color: #1E3A8A !important;
        color: white !important;
        font-weight: 600;
        padding: 12px !important;
    }
    .stDataFrame td {
        padding: 10px !important;
    }
    .stDataFrame tr:nth-child(even) {
        background-color: #f8f9fa;
    }
    .sidebar .sidebar-content {
        background-color: #f8f9fa;
    }
    [data-testid="stSidebar"] {
        background-color: #f8f9fa;
    }
    .insight-card {
        background-color: #f0f7ff;
        border-left: 5px solid #3b82f6;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .insight-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1e40af;
        margin-bottom: 8px;
    }
    .insight-content {
        color: #334155;
        font-size: 0.95rem;
    }
    .warning-card {
        background-color: #fff1f2;
        border-left: 5px solid #e11d48;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .warning-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #be123c;
        margin-bottom: 8px;
    }
    .tab-container {
        margin-top: 1rem;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: white;
        border-radius: 4px 4px 0px 0px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #1E3A8A;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# Define the scopes
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]

# Authentication function
def authenticate():
    """Authentication using OAuth token"""
    try:
        creds = None
        
        # Check if token.json exists first
        if os.path.exists('token.json'):
            try:
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i token.json: {e}")
        # Otherwise create it from the environment variable or Streamlit secrets
        elif 'GOOGLE_TOKEN_JSON' in os.environ:
            try:
                token_info = os.environ.get('GOOGLE_TOKEN_JSON')
                with open('token.json', 'w') as f:
                    f.write(token_info)
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i t·ª´ bi·∫øn m√¥i tr∆∞·ªùng: {e}")
        elif 'GOOGLE_TOKEN_JSON' in st.secrets:
            try:
                token_info = st.secrets['GOOGLE_TOKEN_JSON']
                with open('token.json', 'w') as f:
                    f.write(token_info)
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                st.error(f"L·ªói khi t·∫£i t·ª´ Streamlit secrets: {e}")
        else:
            st.error("‚ùå Kh√¥ng t√¨m th·∫•y file token.json ho·∫∑c GOOGLE_TOKEN_JSON")
            return None
        
        # Refresh token if expired
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
                with open('token.json', 'w') as token:
                    token.write(creds.to_json())
            except Exception as e:
                st.error(f"L·ªói khi l√†m m·ªõi token: {e}")
                
        # Return authorized client
        if creds:
            return gspread.authorize(creds)
        else:
            return None
    
    except Exception as e:
        st.error(f"‚ùå L·ªói x√°c th·ª±c: {str(e)}")
        return None

# Function to load AQL data
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the ID AQL worksheet
            try:
                worksheet = spreadsheet.worksheet('ID AQL')
            except gspread.exceptions.WorksheetNotFound:
                connection_status.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'ID AQL'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Basic data cleaning
            # Convert date columns to datetime if needed
            if "Ng√†y SX" in df.columns:
                try:
                    df["Ng√†y SX"] = pd.to_datetime(df["Ng√†y SX"], format="%d/%m/%Y", errors='coerce')
                    df["Production_Month"] = df["Ng√†y SX"].dt.strftime("%m/%Y")
                    df["Production_Date"] = df["Ng√†y SX"]
                except Exception as e:
                    connection_status.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x·ª≠ l√Ω c·ªôt ng√†y: {e}")
            
            # Make sure numeric columns are properly typed
            if "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" in df.columns:
                df["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"] = pd.to_numeric(df["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"], errors='coerce').fillna(0)
            
            # Ensure Line column is converted to string for consistent filtering
            if "Line" in df.columns:
                df["Line"] = df["Line"].astype(str)
            
            # Process shift information based on hour
            if "Gi·ªù" in df.columns:
                # Convert to numeric if not already
                df["Gi·ªù"] = pd.to_numeric(df["Gi·ªù"], errors='coerce')
                
                # Define a function to map hours to shifts
                def map_hour_to_shift(hour):
                    if pd.isna(hour):
                        return "Unknown"
                    hour = float(hour)
                    if 6 <= hour < 14:
                        return "1"
                    elif 14 <= hour < 22:
                        return "2"
                    else:  # 22-24 or 0-6
                        return "3"
                
                # Apply the mapping function
                df["Shift"] = df["Gi·ªù"].apply(map_hour_to_shift)
                
                # Convert Shift to string
                df["Shift"] = df["Shift"].astype(str)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói truy c·∫≠p b·∫£ng d·ªØ li·ªáu AQL: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu AQL: {str(e)}")
        return pd.DataFrame()

# Function to load production data (S·∫£n l∆∞·ª£ng)
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_production_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data - same spreadsheet, different worksheet)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the S·∫£n l∆∞·ª£ng worksheet
            try:
                worksheet = spreadsheet.worksheet('S·∫£n l∆∞·ª£ng')
            except gspread.exceptions.WorksheetNotFound:
                connection_status.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'S·∫£n l∆∞·ª£ng'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Basic data cleaning
            # Convert date columns to datetime if needed
            if "Ng√†y" in df.columns:
                try:
                    df["Ng√†y"] = pd.to_datetime(df["Ng√†y"], format="%d/%m/%Y", errors='coerce')
                    df["Production_Month"] = df["Ng√†y"].dt.strftime("%m/%Y")
                    df["Production_Date"] = df["Ng√†y"]
                except Exception as e:
                    connection_status.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x·ª≠ l√Ω c·ªôt ng√†y: {e}")
            
            # Make sure numeric columns are properly typed
            if "S·∫£n l∆∞·ª£ng" in df.columns:
                df["S·∫£n l∆∞·ª£ng"] = pd.to_numeric(df["S·∫£n l∆∞·ª£ng"], errors='coerce').fillna(0)
            
            # Ensure Line column is converted to string for consistent filtering
            if "Line" in df.columns:
                df["Line"] = df["Line"].astype(str)
            
            # Ensure Ca column is properly formatted
            if "Ca" in df.columns:
                df["Ca"] = df["Ca"].astype(str)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói truy c·∫≠p b·∫£ng d·ªØ li·ªáu s·∫£n l∆∞·ª£ng: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu s·∫£n l∆∞·ª£ng: {str(e)}")
        return pd.DataFrame()

# Function to load AQL g√≥i data for defect name mapping
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_goi_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            
            # Get the AQL g√≥i worksheet
            try:
                worksheet = spreadsheet.worksheet('AQL g√≥i')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'AQL g√≥i'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Return only defect code and name columns if they exist
            defect_code_col = next((col for col in df.columns if "code" in col.lower()), None)
            defect_name_col = next((col for col in df.columns if "name" in col.lower() or "t√™n" in col.lower()), None)
            
            if defect_code_col and defect_name_col:
                return df[[defect_code_col, defect_name_col]]
            else:
                # If specific columns not found, return the full dataframe
                return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói truy c·∫≠p b·∫£ng AQL g√≥i: {str(e)}")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu AQL g√≥i: {str(e)}")
        return pd.DataFrame()

# Function to load AQL T√¥ ly data for defect name mapping
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_to_ly_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            
            # Get the AQL T√¥ ly worksheet
            try:
                worksheet = spreadsheet.worksheet('AQL T√¥ ly')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'AQL T√¥ ly'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Return only defect code and name columns if they exist
            defect_code_col = next((col for col in df.columns if "code" in col.lower()), None)
            defect_name_col = next((col for col in df.columns if "name" in col.lower() or "t√™n" in col.lower()), None)
            
            if defect_code_col and defect_name_col:
                return df[[defect_code_col, defect_name_col]]
            else:
                # If specific columns not found, return the full dataframe
                return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói truy c·∫≠p b·∫£ng AQL T√¥ ly: {str(e)}")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu AQL T√¥ ly: {str(e)}")
        return pd.DataFrame()

# Function to calculate TEM V√ÄNG - FIXED
def calculate_tem_vang(aql_df, production_df):
    """Calculate TEM V√ÄNG by combining AQL hold data with production volume data"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or production_df.empty:
            st.error("‚ùå Kh√¥ng th·ªÉ t√≠nh TEM V√ÄNG - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        prod_copy = production_df.copy()
        
        # Group AQL data by date and line to get total hold quantities
        if "Production_Date" in aql_copy.columns and "Line" in aql_copy.columns and "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" in aql_copy.columns:
            # Make sure we don't count rows with no hold quantity
            aql_copy.loc[aql_copy["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].isna(), "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"] = 0
            
            aql_grouped = aql_copy.groupby(["Production_Date", "Line"])["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index()
            aql_grouped.columns = ["Date", "Line", "Hold_Quantity"]
            
            # Display sample for debugging
            if not aql_grouped.empty:
                st.sidebar.write("AQL data grouped by date/line:", aql_grouped.head(3).to_dict('records'))
        else:
            missing_cols = []
            if "Production_Date" not in aql_copy.columns:
                missing_cols.append("Production_Date")
            if "Line" not in aql_copy.columns:
                missing_cols.append("Line")
            if "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" not in aql_copy.columns:
                missing_cols.append("S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)")
            
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt c·∫ßn thi·∫øt trong d·ªØ li·ªáu AQL ƒë·ªÉ t√≠nh TEM V√ÄNG: {', '.join(missing_cols)}")
            return pd.DataFrame()
        
        # Group production data by date and line to get total production volumes
        if "Production_Date" in prod_copy.columns and "Line" in prod_copy.columns and "S·∫£n l∆∞·ª£ng" in prod_copy.columns:
            # Make sure we don't count rows with no production volume
            prod_copy.loc[prod_copy["S·∫£n l∆∞·ª£ng"].isna(), "S·∫£n l∆∞·ª£ng"] = 0
            
            prod_grouped = prod_copy.groupby(["Production_Date", "Line"])["S·∫£n l∆∞·ª£ng"].sum().reset_index()
            prod_grouped.columns = ["Date", "Line", "Production_Volume"]
            
            # Display sample for debugging
            if not prod_grouped.empty:
                st.sidebar.write("Production data grouped by date/line:", prod_grouped.head(3).to_dict('records'))
        else:
            missing_cols = []
            if "Production_Date" not in prod_copy.columns:
                missing_cols.append("Production_Date")
            if "Line" not in prod_copy.columns:
                missing_cols.append("Line")
            if "S·∫£n l∆∞·ª£ng" not in prod_copy.columns:
                missing_cols.append("S·∫£n l∆∞·ª£ng")
            
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt c·∫ßn thi·∫øt trong d·ªØ li·ªáu s·∫£n l∆∞·ª£ng ƒë·ªÉ t√≠nh TEM V√ÄNG: {', '.join(missing_cols)}")
            return pd.DataFrame()
        
        # Merge the grouped data
        tem_vang_df = pd.merge(aql_grouped, prod_grouped, on=["Date", "Line"], how="inner")
        
        # Display sample for debugging
        if not tem_vang_df.empty:
            st.sidebar.write("Merged TEM VANG data:", tem_vang_df.head(3).to_dict('records'))
        
        # Calculate TEM V√ÄNG percentage
        tem_vang_df["TEM_VANG"] = (tem_vang_df["Hold_Quantity"] / tem_vang_df["Production_Volume"]) * 100
        
        # Add month column for filtering
        tem_vang_df["Production_Month"] = tem_vang_df["Date"].dt.strftime("%m/%Y")
        
        return tem_vang_df
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t√≠nh to√°n TEM V√ÄNG: {str(e)}")
        return pd.DataFrame()

# Function to calculate TEM V√ÄNG by shift - FIXED
def calculate_tem_vang_by_shift(aql_df, production_df):
    """Calculate TEM V√ÄNG by shift using AQL and production data"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or production_df.empty:
            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh TEM V√ÄNG theo ca - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        prod_copy = production_df.copy()
        
        # Display column names for debugging
        st.sidebar.write("AQL columns for shift:", aql_copy.columns.tolist())
        st.sidebar.write("Production columns for shift:", prod_copy.columns.tolist())
        
        # Ensure we have all required columns
        required_aql_cols = ["Production_Date", "Line", "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"]
        missing_aql_cols = [col for col in required_aql_cols if col not in aql_copy.columns]
        
        required_prod_cols = ["Production_Date", "Line", "S·∫£n l∆∞·ª£ng", "Ca"]
        missing_prod_cols = [col for col in required_prod_cols if col not in prod_copy.columns]
        
        if missing_aql_cols:
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt trong d·ªØ li·ªáu AQL ƒë·ªÉ t√≠nh TEM V√ÄNG theo ca: {', '.join(missing_aql_cols)}")
            return pd.DataFrame()
        
        if missing_prod_cols:
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt trong d·ªØ li·ªáu s·∫£n l∆∞·ª£ng ƒë·ªÉ t√≠nh TEM V√ÄNG theo ca: {', '.join(missing_prod_cols)}")
            return pd.DataFrame()
        
        # Ensure we have shift information for AQL data
        if "Shift" not in aql_copy.columns:
            # If we don't have Shift column but have Gi·ªù, derive Shift from Gi·ªù
            if "Gi·ªù" in aql_copy.columns:
                # Convert Gi·ªù to numeric
                aql_copy["Gi·ªù"] = pd.to_numeric(aql_copy["Gi·ªù"], errors='coerce')
                
                # Define shift mapping function
                def hour_to_shift(hour):
                    if pd.isna(hour):
                        return "Unknown"
                    hour = float(hour)
                    if 6 <= hour < 14:
                        return "1"
                    elif 14 <= hour < 22:
                        return "2"
                    else:  # 22-24 or 0-6
                        return "3"
                
                # Apply mapping function
                aql_copy["Shift"] = aql_copy["Gi·ªù"].apply(hour_to_shift)
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ x√°c ƒë·ªãnh ca t·ª´ d·ªØ li·ªáu AQL - thi·∫øu c·ªôt 'Shift' v√† 'Gi·ªù'")
                return pd.DataFrame()
        
        # Ensure Shift is string type for both dataframes
        aql_copy["Shift"] = aql_copy["Shift"].astype(str)
        prod_copy["Ca"] = prod_copy["Ca"].astype(str)
        
        # Group AQL data by date, line, shift
        try:
            aql_grouped = aql_copy.groupby(["Production_Date", "Line", "Shift"])["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index()
            aql_grouped.columns = ["Date", "Line", "Shift", "Hold_Quantity"]
            
            # Display sample for debugging
            if not aql_grouped.empty:
                st.sidebar.write("AQL data grouped by shift:", aql_grouped.head(3).to_dict('records'))
        except Exception as e:
            st.error(f"L·ªói khi nh√≥m d·ªØ li·ªáu AQL theo ca: {e}")
            return pd.DataFrame()
        
        # Group production data by date, line, shift
        try:
            prod_grouped = prod_copy.groupby(["Production_Date", "Line", "Ca"])["S·∫£n l∆∞·ª£ng"].sum().reset_index()
            prod_grouped.columns = ["Date", "Line", "Shift", "Production_Volume"]
            
            # Display sample for debugging
            if not prod_grouped.empty:
                st.sidebar.write("Production data grouped by shift:", prod_grouped.head(3).to_dict('records'))
        except Exception as e:
            st.error(f"L·ªói khi nh√≥m d·ªØ li·ªáu s·∫£n l∆∞·ª£ng theo ca: {e}")
            return pd.DataFrame()
        
        # Merge the data
        tem_vang_shift_df = pd.merge(
            aql_grouped, 
            prod_grouped, 
            on=["Date", "Line", "Shift"],
            how="inner"
        )
        
        # Display merged data for debugging
        if not tem_vang_shift_df.empty:
            st.sidebar.write("Merged shift data:", tem_vang_shift_df.head(3).to_dict('records'))
            st.sidebar.write("Merged shift data shape:", tem_vang_shift_df.shape)
        
        # Calculate TEM V√ÄNG
        tem_vang_shift_df["TEM_VANG"] = (tem_vang_shift_df["Hold_Quantity"] / tem_vang_shift_df["Production_Volume"]) * 100
        
        # Add month column for filtering
        tem_vang_shift_df["Production_Month"] = tem_vang_shift_df["Date"].dt.strftime("%m/%Y")
        
        return tem_vang_shift_df
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t√≠nh to√°n TEM V√ÄNG theo ca: {str(e)}")
        return pd.DataFrame()

# Function to calculate TEM V√ÄNG by shift leader - FIXED to use "T√™n Tr∆∞·ªüng ca" column
def calculate_tem_vang_by_leader(aql_df, production_df):
    """Calculate TEM V√ÄNG by shift leader using AQL and production data"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or production_df.empty:
            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh TEM V√ÄNG theo tr∆∞·ªüng ca - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        prod_copy = production_df.copy()
        
        # Check for required columns
        st.sidebar.write("AQL columns for leader:", aql_copy.columns.tolist())
        st.sidebar.write("Production columns for leader:", prod_copy.columns.tolist())
        
        # Find the columns for T√™n Tr∆∞·ªüng ca in AQL data (FIXED: now looking for "T√™n Tr∆∞·ªüng ca" first)
        ten_truong_ca_col = None
        truong_ca_col = None
        
        for col in aql_copy.columns:
            if "t√™n tr∆∞·ªüng ca" in col.lower():
                ten_truong_ca_col = col
                break
                
        # If we couldn't find "T√™n Tr∆∞·ªüng ca", fall back to "Tr∆∞·ªüng ca"
        if not ten_truong_ca_col:
            for col in aql_copy.columns:
                if "tr∆∞·ªüng ca" in col.lower() and "t√™n" not in col.lower():
                    truong_ca_col = col
                    break
        
        # Find the columns for Ng∆∞·ªùi ph·ª• tr√°ch in production data
        nguoi_phu_trach_col = None
        for col in prod_copy.columns:
            if "ng∆∞·ªùi ph·ª• tr√°ch" in col.lower() or "ph·ª• tr√°ch" in col.lower():
                nguoi_phu_trach_col = col
                break
        
        # Use T√™n Tr∆∞·ªüng ca if available, otherwise fall back to Tr∆∞·ªüng ca
        leader_col = ten_truong_ca_col if ten_truong_ca_col else truong_ca_col
        
        if not leader_col:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'T√™n Tr∆∞·ªüng ca' ho·∫∑c 'Tr∆∞·ªüng ca' trong d·ªØ li·ªáu AQL")
            return pd.DataFrame()
        
        if not nguoi_phu_trach_col:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt ng∆∞·ªùi ph·ª• tr√°ch trong d·ªØ li·ªáu s·∫£n l∆∞·ª£ng")
            return pd.DataFrame()
        
        # Display found columns for debugging
        st.sidebar.write(f"Found Leader column: {leader_col}")
        st.sidebar.write(f"Found Ng∆∞·ªùi ph·ª• tr√°ch column: {nguoi_phu_trach_col}")
        
        # Ensure required columns exist
        required_aql_cols = ["Production_Date", "Line", "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"]
        missing_aql_cols = [col for col in required_aql_cols if col not in aql_copy.columns]
        
        required_prod_cols = ["Production_Date", "Line", "S·∫£n l∆∞·ª£ng"]
        missing_prod_cols = [col for col in required_prod_cols if col not in prod_copy.columns]
        
        if missing_aql_cols:
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt trong d·ªØ li·ªáu AQL ƒë·ªÉ t√≠nh TEM V√ÄNG theo tr∆∞·ªüng ca: {', '.join(missing_aql_cols)}")
            return pd.DataFrame()
        
        if missing_prod_cols:
            st.warning(f"‚ö†Ô∏è Thi·∫øu c·ªôt trong d·ªØ li·ªáu s·∫£n l∆∞·ª£ng ƒë·ªÉ t√≠nh TEM V√ÄNG theo tr∆∞·ªüng ca: {', '.join(missing_prod_cols)}")
            return pd.DataFrame()
        
        # Display unique leader values for debugging
        st.sidebar.write("Unique Leader values:", aql_copy[leader_col].dropna().unique())
        st.sidebar.write("Unique Ng∆∞·ªùi ph·ª• tr√°ch values:", prod_copy[nguoi_phu_trach_col].dropna().unique())
        
        # Group AQL data by date, line, leader
        try:
            aql_grouped = aql_copy.groupby(["Production_Date", "Line", leader_col])["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index()
            aql_grouped.columns = ["Date", "Line", "Leader", "Hold_Quantity"]
            
            # Display sample for debugging
            if not aql_grouped.empty:
                st.sidebar.write("AQL data grouped by leader:", aql_grouped.head(3).to_dict('records'))
        except Exception as e:
            st.error(f"L·ªói khi nh√≥m d·ªØ li·ªáu AQL theo tr∆∞·ªüng ca: {e}")
            return pd.DataFrame()
        
        # Group production data by date, line, leader
        try:
            prod_grouped = prod_copy.groupby(["Production_Date", "Line", nguoi_phu_trach_col])["S·∫£n l∆∞·ª£ng"].sum().reset_index()
            prod_grouped.columns = ["Date", "Line", "Leader", "Production_Volume"]
            
            # Display sample for debugging
            if not prod_grouped.empty:
                st.sidebar.write("Production data grouped by leader:", prod_grouped.head(3).to_dict('records'))
        except Exception as e:
            st.error(f"L·ªói khi nh√≥m d·ªØ li·ªáu s·∫£n l∆∞·ª£ng theo ng∆∞·ªùi ph·ª• tr√°ch: {e}")
            return pd.DataFrame()
        
        # Standardize leader names for better matching
        aql_grouped["Leader"] = aql_grouped["Leader"].astype(str).str.strip().str.lower()
        prod_grouped["Leader"] = prod_grouped["Leader"].astype(str).str.strip().str.lower()
        
        # Merge the data
        tem_vang_leader_df = pd.merge(
            aql_grouped, 
            prod_grouped, 
            on=["Date", "Line", "Leader"],
            how="inner"
        )
        
        # Display merged data for debugging
        if not tem_vang_leader_df.empty:
            st.sidebar.write("Merged leader data:", tem_vang_leader_df.head(3).to_dict('records'))
            st.sidebar.write("Merged leader data shape:", tem_vang_leader_df.shape)
        else:
            st.sidebar.write("No matching leader data found after merge")
        
        # Calculate TEM V√ÄNG
        tem_vang_leader_df["TEM_VANG"] = (tem_vang_leader_df["Hold_Quantity"] / tem_vang_leader_df["Production_Volume"]) * 100
        
        # Add month column for filtering
        tem_vang_leader_df["Production_Month"] = tem_vang_leader_df["Date"].dt.strftime("%m/%Y")
        
        return tem_vang_leader_df
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t√≠nh to√°n TEM V√ÄNG theo tr∆∞·ªüng ca: {str(e)}")
        return pd.DataFrame()

# Function to calculate TEM V√ÄNG by hour - REVISED
def calculate_tem_vang_by_hour(aql_df, production_df):
    """Calculate TEM V√ÄNG by hour using AQL and production data"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or production_df.empty:
            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ t√≠nh TEM V√ÄNG theo gi·ªù - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        prod_copy = production_df.copy()
        
        # Check if Gi·ªù column exists
        if "Gi·ªù" not in aql_copy.columns:
            st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt 'Gi·ªù' trong d·ªØ li·ªáu AQL ƒë·ªÉ t√≠nh TEM V√ÄNG theo gi·ªù")
            return pd.DataFrame()
        
        # Check if we have shift column in production data
        if "Ca" not in prod_copy.columns:
            st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt 'Ca' trong d·ªØ li·ªáu s·∫£n l∆∞·ª£ng ƒë·ªÉ t√≠nh TEM V√ÄNG theo gi·ªù")
            return pd.DataFrame()
        
        # Ensure Gi·ªù is numeric
        aql_copy["Gi·ªù"] = pd.to_numeric(aql_copy["Gi·ªù"], errors='coerce')
        
        # Map hours to shifts
        hour_to_shift = {
            h: "1" if 6 <= h < 14 else ("2" if 14 <= h < 22 else "3")
            for h in range(24)
        }
        
        # Add shift column based on hour
        aql_copy["Shift"] = aql_copy["Gi·ªù"].map(lambda h: hour_to_shift.get(h, "Unknown") if pd.notna(h) else "Unknown")
        
        # Group AQL data by hour, ignoring date and line to get aggregated values
        aql_hour_grouped = aql_copy.groupby("Gi·ªù")["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index()
        aql_hour_grouped.columns = ["Hour", "Hold_Quantity"]
        
        # Add shift column to the grouped data
        aql_hour_grouped["Shift"] = aql_hour_grouped["Hour"].map(lambda h: hour_to_shift.get(h, "Unknown") if pd.notna(h) else "Unknown")
        
        # Group production data by shift (Ca)
        prod_copy["Ca"] = prod_copy["Ca"].astype(str)
        shift_production = prod_copy.groupby("Ca")["S·∫£n l∆∞·ª£ng"].sum().reset_index()
        shift_production.columns = ["Shift", "Production_Volume"]
        
        # Display for debugging
        st.sidebar.write("Hour-grouped AQL data:", aql_hour_grouped.head().to_dict('records'))
        st.sidebar.write("Shift-grouped production data:", shift_production.to_dict('records'))
        
        # Define hours per shift for distribution
        hours_per_shift = {
            "1": 8,  # 6-14 (8 hours)
            "2": 8,  # 14-22 (8 hours)
            "3": 8   # 22-6 (8 hours)
        }
        
        # Merge to get production volume for each hour
        tem_vang_hour_df = pd.merge(
            aql_hour_grouped,
            shift_production,
            on="Shift",
            how="left"
        )
        
        # Display for debugging
        st.sidebar.write("Merged hour data before calculations:", tem_vang_hour_df.head().to_dict('records'))
        
        # Calculate hourly production by dividing shift production by hours per shift
        tem_vang_hour_df["Hourly_Production"] = tem_vang_hour_df.apply(
            lambda row: row["Production_Volume"] / hours_per_shift.get(row["Shift"], 8) 
            if pd.notna(row["Shift"]) and pd.notna(row["Production_Volume"]) and row["Production_Volume"] > 0
            else 0,
            axis=1
        )
        
        # Calculate TEM V√ÄNG percentage
        tem_vang_hour_df["TEM_VANG"] = tem_vang_hour_df.apply(
            lambda row: (row["Hold_Quantity"] / row["Hourly_Production"]) * 100 
            if row["Hourly_Production"] > 0
            else 0,
            axis=1
        )
        
        # Sort by hour
        tem_vang_hour_df = tem_vang_hour_df.sort_values("Hour")
        
        return tem_vang_hour_df
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t√≠nh to√°n TEM V√ÄNG theo gi·ªù: {str(e)}")
        st.error(f"Chi ti·∫øt l·ªói: {e}")
        return pd.DataFrame()

# Function to map defect codes to defect names
def map_defect_codes_to_names(aql_df, aql_goi_df, aql_to_ly_df):
    """Map defect codes to proper defect names based on line number"""
    try:
        # Check if dataframes are empty
        if aql_df.empty:
            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ √°nh x·∫° m√£ l·ªói - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create a copy to avoid modifying the original
        df = aql_df.copy()
        
        # Create a Defect_Name column
        df["Defect_Name"] = ""
        
        # Get defect code column from AQL data
        defect_code_col = next((col for col in df.columns if "defect code" in col.lower()), None)
        actual_defect_col = next((col for col in df.columns if "actual defect" in col.lower()), None)
        
        if not defect_code_col:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt m√£ l·ªói trong d·ªØ li·ªáu AQL")
            return df
        
        # If we already have actual defect column, use it directly
        if actual_defect_col:
            df["Defect_Name"] = df[actual_defect_col]
            return df
        
        # Get mapping columns from AQL g√≥i and AQL T√¥ ly data
        if not aql_goi_df.empty:
            goi_code_col = aql_goi_df.columns[0]
            goi_name_col = aql_goi_df.columns[1]
            
            # Create a mapping dictionary for g√≥i
            goi_mapping = dict(zip(aql_goi_df[goi_code_col], aql_goi_df[goi_name_col]))
        else:
            goi_mapping = {}
        
        if not aql_to_ly_df.empty:
            to_ly_code_col = aql_to_ly_df.columns[0]
            to_ly_name_col = aql_to_ly_df.columns[1]
            
            # Create a mapping dictionary for t√¥ ly
            to_ly_mapping = dict(zip(aql_to_ly_df[to_ly_code_col], aql_to_ly_df[to_ly_name_col]))
        else:
            to_ly_mapping = {}
        
        # Function to map defect code to name based on line
        def map_defect_name(row):
            line = row.get("Line", "")
            defect_code = row.get(defect_code_col, "")
            
            # Convert line to string and check range
            try:
                line_str = str(line)
                if line_str in ["1", "2", "3", "4", "5", "6"]:
                    return goi_mapping.get(defect_code, defect_code)
                elif line_str in ["7", "8"]:
                    return to_ly_mapping.get(defect_code, defect_code)
                else:
                    return defect_code
            except:
                return defect_code
        
        # Apply the mapping function
        df["Defect_Name"] = df.apply(map_defect_name, axis=1)
        
        return df
        
    except Exception as e:
        st.error(f"‚ùå L·ªói √°nh x·∫° m√£ l·ªói: {str(e)}")
        return aql_df.copy()

# Function to analyze defect patterns (revised to use defect names)
def analyze_defect_patterns(aql_df_with_names):
    """Analyze defect patterns in AQL data using defect names instead of codes"""
    try:
        # Check if dataframe is empty
        if aql_df_with_names.empty:
            return {}
        
        # Create copy to avoid modifying original
        df = aql_df_with_names.copy()
        
        # Group by defect name to get frequency
        if "Defect_Name" in df.columns and df["Defect_Name"].nunique() > 0:
            defect_counts = df.groupby("Defect_Name").size().reset_index(name="Count")
            
            # Add additional metric: total hold quantity by defect
            if "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" in df.columns:
                hold_by_defect = df.groupby("Defect_Name")["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index(name="Hold_Quantity")
                defect_counts = pd.merge(defect_counts, hold_by_defect, on="Defect_Name", how="left")
            
            # Sort by count
            defect_counts = defect_counts.sort_values("Count", ascending=False)
            
            # Calculate percentages
            total_defects = defect_counts["Count"].sum()
            defect_counts["Percentage"] = (defect_counts["Count"] / total_defects * 100).round(1)
            defect_counts["Cumulative"] = defect_counts["Percentage"].cumsum()
            
            # Identify top defects (80% by Pareto principle)
            vital_few = defect_counts[defect_counts["Cumulative"] <= 80]
            
            # Group by Line and Defect name to get line-specific patterns
            line_defects = df.groupby(["Line", "Defect_Name"]).size().reset_index(name="Count")
            line_defects_hold = df.groupby(["Line", "Defect_Name"])["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index(name="Hold_Quantity")
            line_defects = pd.merge(line_defects, line_defects_hold, on=["Line", "Defect_Name"], how="left")
            pivot_line_defects = line_defects.pivot(index="Line", columns="Defect_Name", values="Count").fillna(0)
            
            # Return the analysis results
            return {
                "defect_counts": defect_counts,
                "vital_few": vital_few,
                "line_defects": line_defects,
                "pivot_line_defects": pivot_line_defects
            }
        else:
            # If we don't have defect names, try using defect codes
            defect_code_col = next((col for col in df.columns if "defect code" in col.lower()), None)
            
            if defect_code_col:
                st.warning(f"‚ö†Ô∏è S·ª≠ d·ª•ng m√£ l·ªói thay v√¨ t√™n l·ªói cho ph√¢n t√≠ch Pareto")
                
                defect_counts = df.groupby(defect_code_col).size().reset_index(name="Count")
                
                # Add additional metric: total hold quantity by defect
                if "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" in df.columns:
                    hold_by_defect = df.groupby(defect_code_col)["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index(name="Hold_Quantity")
                    defect_counts = pd.merge(defect_counts, hold_by_defect, on=defect_code_col, how="left")
                
                # Sort by count
                defect_counts = defect_counts.sort_values("Count", ascending=False)
                
                # Calculate percentages
                total_defects = defect_counts["Count"].sum()
                defect_counts["Percentage"] = (defect_counts["Count"] / total_defects * 100).round(1)
                defect_counts["Cumulative"] = defect_counts["Percentage"].cumsum()
                
                # Identify top defects (80% by Pareto principle)
                vital_few = defect_counts[defect_counts["Cumulative"] <= 80]
                
                # Group by Line and Defect code to get line-specific patterns
                line_defects = df.groupby(["Line", defect_code_col]).size().reset_index(name="Count")
                line_defects_hold = df.groupby(["Line", defect_code_col])["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum().reset_index(name="Hold_Quantity")
                line_defects = pd.merge(line_defects, line_defects_hold, on=["Line", defect_code_col], how="left")
                pivot_line_defects = line_defects.pivot(index="Line", columns=defect_code_col, values="Count").fillna(0)
                
                # Rename columns for compatibility
                defect_counts.rename(columns={defect_code_col: "Defect_Name"}, inplace=True)
                
                # Return the analysis results
                return {
                    "defect_counts": defect_counts,
                    "vital_few": vital_few,
                    "line_defects": line_defects,
                    "pivot_line_defects": pivot_line_defects
                }
            else:
                st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt t√™n l·ªói ho·∫∑c m√£ l·ªói trong d·ªØ li·ªáu AQL ƒë·ªÉ ph√¢n t√≠ch m·∫´u l·ªói")
                return {}
            
    except Exception as e:
        st.error(f"‚ùå L·ªói ph√¢n t√≠ch m·∫´u l·ªói: {str(e)}")
        return {}

# Load all data needed - FIXED to ensure all dictionary keys are initialized
@st.cache_data(ttl=600)  # Cache the combined data for 10 minutes
def load_all_data():
    """Load and prepare all required data"""
    
    # Initialize an empty result dictionary with all required keys
    result = {
        "aql_data": pd.DataFrame(),
        "aql_data_with_names": pd.DataFrame(),
        "production_data": pd.DataFrame(),
        "tem_vang_data": pd.DataFrame(),
        "tem_vang_shift_df": pd.DataFrame(),  # Note the key name matches what's used later
        "tem_vang_leader_df": pd.DataFrame(),
        "tem_vang_hour_data": pd.DataFrame(),
        "defect_patterns": {}
    }
    
    # Load raw data
    aql_df = load_aql_data()
    production_df = load_production_data()
    aql_goi_df = load_aql_goi_data()  
    aql_to_ly_df = load_aql_to_ly_data()
    
    # Update the result dictionary with the loaded data
    result["aql_data"] = aql_df
    result["production_data"] = production_df
    
    # Only proceed with further processing if we have the necessary data
    if not aql_df.empty and not production_df.empty:
        # Map defect codes to names
        aql_df_with_names = map_defect_codes_to_names(aql_df, aql_goi_df, aql_to_ly_df)
        result["aql_data_with_names"] = aql_df_with_names
        
        # Calculate TEM V√ÄNG metrics
        result["tem_vang_data"] = calculate_tem_vang(aql_df, production_df)
        result["tem_vang_shift_df"] = calculate_tem_vang_by_shift(aql_df, production_df)
        result["tem_vang_leader_df"] = calculate_tem_vang_by_leader(aql_df, production_df)
        result["tem_vang_hour_data"] = calculate_tem_vang_by_hour(aql_df, production_df)
        
        # Analyze defect patterns with names
        result["defect_patterns"] = analyze_defect_patterns(aql_df_with_names)
    
    return result

# Title and description
st.markdown('<div class="main-header">B√°o c√°o ch·∫•t l∆∞·ª£ng CF MMB</div>', unsafe_allow_html=True)

# Toggle debug mode
debug_mode = st.sidebar.checkbox("Debug Mode", value=True)

# Load all data
data = load_all_data()

# Check if key dataframes are empty
if data["aql_data"].empty or data["production_data"].empty:
    st.warning("‚ö†Ô∏è Thi·∫øu d·ªØ li·ªáu c·∫ßn thi·∫øt. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Google Sheet.")
    # Still continue rendering with available data

# Create a sidebar for filters
with st.sidebar:
    if not debug_mode:
        st.markdown("<h2 style='text-align: center; color: #1E3A8A;'>B·ªô l·ªçc</h2>", unsafe_allow_html=True)
    
    # Initialize filtered dataframes
    filtered_aql_df = data["aql_data_with_names"].copy()
    filtered_tem_vang_df = data["tem_vang_data"].copy()
    filtered_tem_vang_shift_df = data["tem_vang_shift_df"].copy()
    filtered_tem_vang_leader_df = data["tem_vang_leader_df"].copy()
    filtered_tem_vang_hour_df = data["tem_vang_hour_data"].copy()
    
    # Date filter for production data
    if not debug_mode:
        st.subheader("Kho·∫£ng th·ªùi gian s·∫£n xu·∫•t")
    
    # Get min and max dates from AQL data
    if not data["aql_data"].empty and "Production_Date" in data["aql_data"].columns:
        min_prod_date = data["aql_data"]["Production_Date"].min().date()
        max_prod_date = data["aql_data"]["Production_Date"].max().date()
    else:
        min_prod_date = datetime.now().date() - timedelta(days=365)
        max_prod_date = datetime.now().date()
    
    # Create date range selector for production data
    prod_date_range = st.date_input(
        "Ch·ªçn kho·∫£ng th·ªùi gian s·∫£n xu·∫•t",
        value=(min_prod_date, max_prod_date),
        min_value=min_prod_date - timedelta(days=365),
        max_value=max_prod_date + timedelta(days=30)
    )
    
    # Apply production date filter if a range is selected
    if len(prod_date_range) == 2:
        start_date, end_date = prod_date_range
        
        # Convert to datetime for filtering
        start_datetime = pd.Timestamp(start_date)
        end_datetime = pd.Timestamp(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
        
        # Apply to AQL data
        if not filtered_aql_df.empty and "Production_Date" in filtered_aql_df.columns:
            filtered_aql_df = filtered_aql_df[
                (filtered_aql_df["Production_Date"] >= start_datetime) & 
                (filtered_aql_df["Production_Date"] <= end_datetime)
            ]
        
        # Apply to TEM V√ÄNG data
        if not filtered_tem_vang_df.empty and "Date" in filtered_tem_vang_df.columns:
            filtered_tem_vang_df = filtered_tem_vang_df[
                (filtered_tem_vang_df["Date"] >= start_datetime) & 
                (filtered_tem_vang_df["Date"] <= end_datetime)
            ]
            
        # Apply to TEM V√ÄNG by shift data
        if not filtered_tem_vang_shift_df.empty and "Date" in filtered_tem_vang_shift_df.columns:
            filtered_tem_vang_shift_df = filtered_tem_vang_shift_df[
                (filtered_tem_vang_shift_df["Date"] >= start_datetime) & 
                (filtered_tem_vang_shift_df["Date"] <= end_datetime)
            ]
            
        # Apply to TEM V√ÄNG by leader data
        if not filtered_tem_vang_leader_df.empty and "Date" in filtered_tem_vang_leader_df.columns:
            filtered_tem_vang_leader_df = filtered_tem_vang_leader_df[
                (filtered_tem_vang_leader_df["Date"] >= start_datetime) & 
                (filtered_tem_vang_leader_df["Date"] <= end_datetime)
            ]
    
    # Line filter - Always include all lines from 1 to 8 regardless of data
    all_lines = ["T·∫•t c·∫£"] + [str(i) for i in range(1, 9)]
    selected_line = st.selectbox("üè≠ Ch·ªçn Line s·∫£n xu·∫•t", all_lines)
    
    if selected_line != "T·∫•t c·∫£":
        # Apply filter to dataframes if the line exists in them
        if not filtered_tem_vang_df.empty and "Line" in filtered_tem_vang_df.columns:
            filtered_tem_vang_df = filtered_tem_vang_df[filtered_tem_vang_df["Line"] == selected_line]
        
        if not filtered_aql_df.empty and "Line" in filtered_aql_df.columns:
            filtered_aql_df = filtered_aql_df[filtered_aql_df["Line"] == selected_line]
            
        if not filtered_tem_vang_shift_df.empty and "Line" in filtered_tem_vang_shift_df.columns:
            filtered_tem_vang_shift_df = filtered_tem_vang_shift_df[filtered_tem_vang_shift_df["Line"] == selected_line]
            
        if not filtered_tem_vang_leader_df.empty and "Line" in filtered_tem_vang_leader_df.columns:
            filtered_tem_vang_leader_df = filtered_tem_vang_leader_df[filtered_tem_vang_leader_df["Line"] == selected_line]
    
    # Shift filter
    all_shifts = ["T·∫•t c·∫£", "1", "2", "3"]
    selected_shift = st.selectbox("‚è±Ô∏è Ch·ªçn Ca", all_shifts)
    
    if selected_shift != "T·∫•t c·∫£":
        # Apply filter to shift-related dataframes
        if not filtered_tem_vang_shift_df.empty and "Shift" in filtered_tem_vang_shift_df.columns:
            filtered_tem_vang_shift_df = filtered_tem_vang_shift_df[filtered_tem_vang_shift_df["Shift"] == selected_shift]
        
        if not filtered_aql_df.empty and "Shift" in filtered_aql_df.columns:
            filtered_aql_df = filtered_aql_df[filtered_aql_df["Shift"] == selected_shift]
    
    # Refresh button
    if st.button("üîÑ L√†m m·ªõi d·ªØ li·ªáu", use_container_width=True):
        st.cache_data.clear()
        st.experimental_rerun()
    
    # Show last update time
    st.markdown(f"**C·∫≠p nh·∫≠t cu·ªëi:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    
    # Add auto-refresh checkbox
    auto_refresh = st.checkbox("‚è±Ô∏è T·ª± ƒë·ªông l√†m m·ªõi (5p)", value=False)

# Production Quality Analysis (just the first tab)
st.markdown('<div class="sub-header">T·ªïng quan ch·∫•t l∆∞·ª£ng s·∫£n xu·∫•t</div>', unsafe_allow_html=True)

# Key metrics row
metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)

with metrics_col1:
    if not filtered_tem_vang_df.empty:
        avg_tem_vang = filtered_tem_vang_df["TEM_VANG"].mean()
        
        # Target TEM V√ÄNG now depends on line selection
        if selected_line in ["7", "8"]:
            tem_target = 2.18
        elif selected_line in ["1", "2", "3", "4", "5", "6"]:
            tem_target = 0.29
        else:
            tem_target = 0.41  # Total/all lines target
            
        tem_delta = avg_tem_vang - tem_target
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">TEM V√ÄNG trung b√¨nh</div>
            <div class="metric-value">{avg_tem_vang:.2f}%</div>
            <div style="color: {'red' if tem_delta > 0 else 'green'};">
                {f"{tem_delta:.2f}% {'cao h∆°n' if tem_delta > 0 else 'th·∫•p h∆°n'} m·ª•c ti√™u"}
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">TEM V√ÄNG trung b√¨nh</div>
            <div class="metric-value">N/A</div>
        </div>
        """, unsafe_allow_html=True)

with metrics_col2:
    if not filtered_tem_vang_df.empty:
        total_hold = filtered_tem_vang_df["Hold_Quantity"].sum()
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">T·ªïng s·ªë l∆∞·ª£ng hold</div>
            <div class="metric-value">{total_hold:,.0f}</div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">T·ªïng s·ªë l∆∞·ª£ng hold</div>
            <div class="metric-value">N/A</div>
        </div>
        """, unsafe_allow_html=True)

with metrics_col3:
    if "defect_patterns" in data and "defect_counts" in data["defect_patterns"]:
        defect_types = len(data["defect_patterns"]["defect_counts"])
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">S·ªë lo·∫°i l·ªói</div>
            <div class="metric-value">{defect_types}</div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">S·ªë lo·∫°i l·ªói</div>
            <div class="metric-value">N/A</div>
        </div>
        """, unsafe_allow_html=True)
        
with metrics_col4:
    if not filtered_tem_vang_df.empty:
        total_production = filtered_tem_vang_df["Production_Volume"].sum()
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">T·ªïng s·∫£n l∆∞·ª£ng</div>
            <div class="metric-value">{total_production:,.0f}</div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-title">T·ªïng s·∫£n l∆∞·ª£ng</div>
            <div class="metric-value">N/A</div>
        </div>
        """, unsafe_allow_html=True)

# TEM V√ÄNG Analysis
st.markdown('<div class="sub-header">Ph√¢n t√≠ch TEM V√ÄNG</div>', unsafe_allow_html=True)

tem_col1, tem_col2 = st.columns(2)

with tem_col1:
    # TEM V√ÄNG trend over time
    if not filtered_tem_vang_df.empty:
        try:
            # Group by date to get daily average TEM V√ÄNG
            daily_tem_vang = filtered_tem_vang_df.groupby("Date")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
            
            # Sort by date
            daily_tem_vang = daily_tem_vang.sort_values("Date")
            
            # Create figure
            fig = go.Figure()
            
            # Add TEM V√ÄNG line
            fig.add_trace(go.Scatter(
                x=daily_tem_vang["Date"],
                y=daily_tem_vang["TEM_VANG"],
                mode="lines+markers",
                name="TEM V√ÄNG",
                line=dict(color="royalblue", width=2),
                marker=dict(size=6)
            ))
            
            # Set the appropriate target based on line selection
            if selected_line in ["7", "8"]:
                target_value = 2.18
                target_label = "M·ª•c ti√™u Line 7-8 (2.18%)"
            elif selected_line in ["1", "2", "3", "4", "5", "6"]:
                target_value = 0.29
                target_label = "M·ª•c ti√™u Line 1-6 (0.29%)"
            else:
                target_value = 0.41
                target_label = "M·ª•c ti√™u t·ªïng (0.41%)"
            
            # Add target line
            fig.add_hline(
                y=target_value,
                line_dash="dash",
                line_color="red",
                annotation_text=target_label
            )
            
            # Update layout
            fig.update_layout(
                title="Xu h∆∞·ªõng TEM V√ÄNG theo th·ªùi gian",
                xaxis_title="Ng√†y",
                yaxis_title="TEM V√ÄNG (%)",
                height=350,
                margin=dict(l=40, r=40, t=40, b=40)
            )
            
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì xu h∆∞·ªõng TEM V√ÄNG: {str(e)}")

with tem_col2:
    # TEM V√ÄNG by line
    if not filtered_tem_vang_df.empty:
        try:
            # Group by line to get average TEM V√ÄNG per line
            line_tem_vang = filtered_tem_vang_df.groupby("Line")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
            
            # Sort by Line number
            line_tem_vang = line_tem_vang.sort_values("Line")
            
            # Create figure
            fig = go.Figure()
            
            # Add TEM V√ÄNG bars
            fig.add_trace(go.Bar(
                x=line_tem_vang["Line"],
                y=line_tem_vang["TEM_VANG"],
                name="TEM V√ÄNG",
                marker_color="royalblue",
                text=line_tem_vang["TEM_VANG"].round(2).astype(str) + "%",
                textposition="auto"
            ))
            
            # Add target lines for different line groups
            fig.add_shape(
                type="line",
                x0=-0.5, x1=5.5,  # Lines 1-6
                y0=0.29, y1=0.29,
                line=dict(color="green", width=2, dash="dash"),
                name="Target Lines 1-6"
            )
            
            fig.add_shape(
                type="line",
                x0=5.5, x1=7.5,  # Lines 7-8
                y0=2.18, y1=2.18,
                line=dict(color="red", width=2, dash="dash"),
                name="Target Lines 7-8"
            )
            
            # Add annotations for targets
            fig.add_annotation(
                x=2.5, y=0.29,
                text="Target Lines 1-6: 0.29%",
                showarrow=False,
                yshift=10,
                font=dict(size=10, color="green")
            )
            
            fig.add_annotation(
                x=6.5, y=2.18,
                text="Target Lines 7-8: 2.18%",
                showarrow=False,
                yshift=10,
                font=dict(size=10, color="red")
            )
            
            # Update layout
            fig.update_layout(
                title="TEM V√ÄNG theo Line s·∫£n xu·∫•t",
                xaxis_title="Line",
                yaxis_title="TEM V√ÄNG (%)",
                height=350,
                margin=dict(l=40, r=40, t=40, b=40),
                xaxis=dict(
                    tickmode='array',
                    tickvals=list(range(1, 9)),
                    ticktext=[str(i) for i in range(1, 9)]
                )
            )
            
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì TEM V√ÄNG theo line: {str(e)}")

# TEM V√ÄNG by Shift Analysis
st.markdown('<div class="sub-header">Ph√¢n t√≠ch TEM V√ÄNG theo ca</div>', unsafe_allow_html=True)

shift_col1, shift_col2 = st.columns(2)

with shift_col1:
    # TEM V√ÄNG by shift
    if not filtered_tem_vang_shift_df.empty:
        try:
            # Group by shift to get average TEM V√ÄNG per shift
            shift_tem_vang = filtered_tem_vang_shift_df.groupby("Shift")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
            
            # Sort by shift number
            shift_tem_vang = shift_tem_vang.sort_values("Shift")
            
            # Create figure
            fig = go.Figure()
            
            # Add TEM V√ÄNG bars
            fig.add_trace(go.Bar(
                x=shift_tem_vang["Shift"],
                y=shift_tem_vang["TEM_VANG"],
                name="TEM V√ÄNG",
                marker_color="royalblue",
                text=shift_tem_vang["TEM_VANG"].round(2).astype(str) + "%",
                textposition="auto"
            ))
            
            # Set the appropriate target based on line selection
            if selected_line in ["7", "8"]:
                target_value = 2.18
                target_label = "M·ª•c ti√™u Line 7-8 (2.18%)"
            elif selected_line in ["1", "2", "3", "4", "5", "6"]:
                target_value = 0.29
                target_label = "M·ª•c ti√™u Line 1-6 (0.29%)"
            else:
                target_value = 0.41
                target_label = "M·ª•c ti√™u t·ªïng (0.41%)"
            
            # Add target line
            fig.add_hline(
                y=target_value,
                line_dash="dash",
                line_color="red",
                annotation_text=target_label
            )
            
            # Update layout
            fig.update_layout(
                title="TEM V√ÄNG theo ca",
                xaxis_title="Ca",
                yaxis_title="TEM V√ÄNG (%)",
                height=350,
                margin=dict(l=40, r=40, t=40, b=40)
            )
            
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì TEM V√ÄNG theo ca: {str(e)}")
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu TEM V√ÄNG theo ca")

with shift_col2:
    # TEM V√ÄNG by shift leader
    if not filtered_tem_vang_leader_df.empty:
        try:
            # Group by leader to get average TEM V√ÄNG per leader
            leader_tem_vang = filtered_tem_vang_leader_df.groupby("Leader")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
            
            # Sort by TEM V√ÄNG value
            leader_tem_vang = leader_tem_vang.sort_values("TEM_VANG", ascending=False)
            
            # Create figure
            fig = go.Figure()
            
            # Add TEM V√ÄNG bars
            fig.add_trace(go.Bar(
                x=leader_tem_vang["Leader"],
                y=leader_tem_vang["TEM_VANG"],
                name="TEM V√ÄNG",
                marker_color="royalblue",
                text=leader_tem_vang["TEM_VANG"].round(2).astype(str) + "%",
                textposition="auto"
            ))
            
            # Set the appropriate target based on line selection
            if selected_line in ["7", "8"]:
                target_value = 2.18
                target_label = "M·ª•c ti√™u Line 7-8 (2.18%)"
            elif selected_line in ["1", "2", "3", "4", "5", "6"]:
                target_value = 0.29
                target_label = "M·ª•c ti√™u Line 1-6 (0.29%)"
            else:
                target_value = 0.41
                target_label = "M·ª•c ti√™u t·ªïng (0.41%)"
            
            # Add target line
            fig.add_hline(
                y=target_value,
                line_dash="dash",
                line_color="red",
                annotation_text=target_label
            )
            
            # Update layout
            fig.update_layout(
                title="TEM V√ÄNG theo tr∆∞·ªüng ca",
                xaxis_title="Tr∆∞·ªüng ca",
                yaxis_title="TEM V√ÄNG (%)",
                height=350,
                margin=dict(l=40, r=40, t=40, b=40),
                xaxis_tickangle=-45
            )
            
            st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì TEM V√ÄNG theo tr∆∞·ªüng ca: {str(e)}")
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu TEM V√ÄNG theo tr∆∞·ªüng ca")

# TEM V√ÄNG by Hour Analysis
st.markdown('<div class="sub-header">Ph√¢n t√≠ch TEM V√ÄNG theo gi·ªù</div>', unsafe_allow_html=True)

if not filtered_tem_vang_hour_df.empty:
    try:
        # Sort by hour
        hour_tem_vang = filtered_tem_vang_hour_df.sort_values("Hour")
        
        # Create figure
        fig = go.Figure()
        
        # Add TEM V√ÄNG line
        fig.add_trace(go.Scatter(
            x=hour_tem_vang["Hour"],
            y=hour_tem_vang["TEM_VANG"],
            mode="lines+markers",
            name="TEM V√ÄNG",
            line=dict(color="royalblue", width=2),
            marker=dict(size=6)
        ))
        
        # Set the appropriate target based on line selection
        if selected_line in ["7", "8"]:
            target_value = 2.18
            target_label = "M·ª•c ti√™u Line 7-8 (2.18%)"
        elif selected_line in ["1", "2", "3", "4", "5", "6"]:
            target_value = 0.29
            target_label = "M·ª•c ti√™u Line 1-6 (0.29%)"
        else:
            target_value = 0.41
            target_label = "M·ª•c ti√™u t·ªïng (0.41%)"
        
        # Add target line
        fig.add_hline(
            y=target_value,
            line_dash="dash",
            line_color="red",
            annotation_text=target_label
        )
        
        # Add shift background colors
        fig.add_vrect(
            x0=6, x1=14,
            fillcolor="rgba(135, 206, 250, 0.2)",
            layer="below",
            line_width=0,
            annotation_text="Ca 1 (6-14)",
            annotation_position="top left"
        )
        
        fig.add_vrect(
            x0=14, x1=22,
            fillcolor="rgba(255, 228, 181, 0.2)",
            layer="below",
            line_width=0,
            annotation_text="Ca 2 (14-22)",
            annotation_position="top left"
        )
        
        fig.add_vrect(
            x0=0, x1=6,
            fillcolor="rgba(211, 211, 211, 0.2)",
            layer="below",
            line_width=0,
            annotation_text="Ca 3 (22-6)",
            annotation_position="top left"
        )
        
        fig.add_vrect(
            x0=22, x1=24,
            fillcolor="rgba(211, 211, 211, 0.2)",
            layer="below",
            line_width=0
        )
        
        # Update layout
        fig.update_layout(
            title="Ph√¢n t√≠ch TEM V√ÄNG theo gi·ªù",
            xaxis_title="Gi·ªù",
            yaxis_title="TEM V√ÄNG (%)",
            height=400,
            margin=dict(l=40, r=40, t=40, b=40),
            xaxis=dict(
                tickmode='array',
                tickvals=list(range(0, 24)),
                ticktext=[f"{i:02d}:00" for i in range(0, 24)]
            )
        )
        
        st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì TEM V√ÄNG theo gi·ªù: {str(e)}")
else:
    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu TEM V√ÄNG theo gi·ªù")

# Defect Analysis
st.markdown('<div class="sub-header">Ph√¢n t√≠ch l·ªói theo Line</div>', unsafe_allow_html=True)

defect_col1, defect_col2 = st.columns(2)

with defect_col1:
    # Pareto chart of defects by name
    if "defect_patterns" in data and "defect_counts" in data["defect_patterns"]:
        try:
            defect_counts = data["defect_patterns"]["defect_counts"]
            
            # Create Pareto chart
            fig = make_subplots(specs=[[{"secondary_y": True}]])
            
            # Add bars for defect counts and hold quantity
            fig.add_trace(
                go.Bar(
                    x=defect_counts["Defect_Name"],
                    y=defect_counts["Count"],
                    name="S·ªë l·∫ßn xu·∫•t hi·ªán",
                    marker_color="steelblue"
                ),
                secondary_y=False
            )
            
            if "Hold_Quantity" in defect_counts.columns:
                fig.add_trace(
                    go.Bar(
                        x=defect_counts["Defect_Name"],
                        y=defect_counts["Hold_Quantity"],
                        name="S·ªë l∆∞·ª£ng hold",
                        marker_color="darkred",
                        opacity=0.7
                    ),
                    secondary_y=False
                )
            
            # Add line for cumulative percentage
            fig.add_trace(
                go.Scatter(
                    x=defect_counts["Defect_Name"],
                    y=defect_counts["Cumulative"],
                    name="T√≠ch l≈©y %",
                    mode="lines+markers",
                    marker=dict(color="firebrick"),
                    line=dict(color="firebrick", width=2)
                ),
                secondary_y=True
            )
            
            # Add 80% reference line
            fig.add_hline(
                y=80,
                line_dash="dash",
                line_color="green",
                annotation_text="80% l·ªói",
                secondary_y=True
            )
            
            # Update layout
            fig.update_layout(
                title="Ph√¢n t√≠ch Pareto c√°c lo·∫°i l·ªói",
                xaxis_title="T√™n l·ªói",
                height=400,
                margin=dict(l=40, r=40, t=40, b=80),
                xaxis_tickangle=-45,
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                )
            )
            
            # Set y-axes titles
            fig.update_yaxes(title_text="S·ªë l·ªói / S·ªë l∆∞·ª£ng hold", secondary_y=False)
            fig.update_yaxes(title_text="T√≠ch l≈©y %", secondary_y=True)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Add Pareto analysis insight
            if "vital_few" in data["defect_patterns"]:
                vital_few = data["defect_patterns"]["vital_few"]
                
                st.markdown(f"""
                <div class="insight-card">
                    <div class="insight-title">Ph√¢n t√≠ch Pareto</div>
                    <div class="insight-content">
                        <p>{len(vital_few)} lo·∫°i l·ªói ({len(vital_few)/len(defect_counts)*100:.0f}% t·ªïng s·ªë lo·∫°i) chi·∫øm 80% s·ªë l·ªói.</p>
                        <p>T·∫≠p trung c·∫£i ti·∫øn ch·∫•t l∆∞·ª£ng v√†o: {', '.join(vital_few['Defect_Name'].tolist())}</p>
                    </div>
                </div>
                """, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"L·ªói t·∫°o bi·ªÉu ƒë·ªì Pareto: {str(e)}")

with defect_col2:
    # Defects by line heatmap
    if "defect_patterns" in data and "pivot_line_defects" in data["defect_patterns"]:
        try:
            pivot_df = data["defect_patterns"]["pivot_line_defects"]
            
            if not pivot_df.empty:
                # Create heatmap
                fig = px.imshow(
                    pivot_df,
                    labels=dict(x="T√™n l·ªói", y="Line", color="S·ªë l·ªói"),
                    x=pivot_df.columns,
                    y=pivot_df.index,
                    color_continuous_scale="YlOrRd",
                    aspect="auto"
                )
                
                # Update layout
                fig.update_layout(
                    title="Ph√¢n b·ªë l·ªói theo Line",
                    height=400,
                    margin=dict(l=40, r=40, t=40, b=80),
                    xaxis_tickangle=-45
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu l·ªói ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì nhi·ªát")
        except Exception as e:
            st.error(f"L·ªói t·∫°o b·∫£n ƒë·ªì nhi·ªát l·ªói: {str(e)}")

# Raw data display (for debugging)
if debug_mode:
    st.markdown("### Debugging Information")
    
    # Show TEM V√ÄNG shift breakdown
    if not data["tem_vang_shift_df"].empty:
        st.subheader("TEM V√ÄNG by Shift Raw Data")
        st.dataframe(data["tem_vang_shift_df"])
    
    # Show TEM V√ÄNG leader breakdown
    if not data["tem_vang_leader_df"].empty:
        st.subheader("TEM V√ÄNG by Leader Raw Data")
        st.dataframe(data["tem_vang_leader_df"])
    
    # AQL data sample
    if not data["aql_data"].empty:
        st.subheader("AQL Data Sample (First 5 rows)")
        st.dataframe(data["aql_data"].head())
    
    # Production data sample
    if not data["production_data"].empty:
        st.subheader("Production Data Sample (First 5 rows)")
        st.dataframe(data["production_data"].head())

# Implement auto-refresh if enabled
if auto_refresh:
    time.sleep(300)  # Wait 5 minutes to allow user to view the dashboard
    st.experimental_rerun()  # Then refresh
