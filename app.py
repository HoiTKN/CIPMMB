import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
import gspread
import os
import json
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from plotly.subplots import make_subplots

# Set page configuration with improved styling
st.set_page_config(
    page_title="B√°o c√°o ch·∫•t l∆∞·ª£ng CF MMB",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS to improve the look and feel
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.8rem;
        font-weight: 600;
        color: #1E3A8A;
        margin-top: 1rem;
        margin-bottom: 0.5rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .metric-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #64748b;
    }
    .metric-value {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E3A8A;
    }
    .stDataFrame {
        border-radius: 10px !important;
        overflow: hidden;
    }
    .stDataFrame table {
        border-collapse: collapse;
        width: 100%;
    }
    .stDataFrame th {
        background-color: #1E3A8A !important;
        color: white !important;
        font-weight: 600;
        padding: 12px !important;
    }
    .stDataFrame td {
        padding: 10px !important;
    }
    .stDataFrame tr:nth-child(even) {
        background-color: #f8f9fa;
    }
    .sidebar .sidebar-content {
        background-color: #f8f9fa;
    }
    [data-testid="stSidebar"] {
        background-color: #f8f9fa;
    }
    .insight-card {
        background-color: #f0f7ff;
        border-left: 5px solid #3b82f6;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .insight-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #1e40af;
        margin-bottom: 8px;
    }
    .insight-content {
        color: #334155;
        font-size: 0.95rem;
    }
    .warning-card {
        background-color: #fff1f2;
        border-left: 5px solid #e11d48;
        border-radius: 5px;
        padding: 15px;
        margin-bottom: 15px;
    }
    .warning-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: #be123c;
        margin-bottom: 8px;
    }
    .tab-container {
        margin-top: 1rem;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: white;
        border-radius: 4px 4px 0px 0px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #1E3A8A;
        color: white;
    }
    /* Completely hide authentication status */
    div[data-testid="stExpander"]:has(div:contains("Authentication Status")) {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

# Define the scopes
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]

# Authentication function - Modified to hide the debug expander
def authenticate():
    """Authentication using OAuth token"""
    try:
        creds = None
        
        # Check if token.json exists first
        if os.path.exists('token.json'):
            try:
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                pass
        # Otherwise create it from the environment variable or Streamlit secrets
        elif 'GOOGLE_TOKEN_JSON' in os.environ:
            try:
                token_info = os.environ.get('GOOGLE_TOKEN_JSON')
                with open('token.json', 'w') as f:
                    f.write(token_info)
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                pass
        elif 'GOOGLE_TOKEN_JSON' in st.secrets:
            try:
                token_info = st.secrets['GOOGLE_TOKEN_JSON']
                with open('token.json', 'w') as f:
                    f.write(token_info)
                creds = Credentials.from_authorized_user_file('token.json', SCOPES)
            except Exception as e:
                pass
        else:
            return None
        
        # Refresh token if expired
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
                with open('token.json', 'w') as token:
                    token.write(creds.to_json())
            except Exception as e:
                pass
                
        # Return authorized client
        if creds:
            return gspread.authorize(creds)
        else:
            return None
    
    except Exception as e:
        return None

# Function to load complaint data
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_complaint_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (complaint data)
        sheet_url = "https://docs.google.com/spreadsheets/d/1d6uGPbJV6BsOB6XSB1IS3NhfeaMyMBcaQPvOnNg2yA4/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet and get the worksheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Try to get the "Integrated_Data" worksheet
            try:
                worksheet = spreadsheet.worksheet('Integrated_Data')
            except gspread.exceptions.WorksheetNotFound:
                # Fall back to first worksheet if Integrated_Data doesn't exist
                worksheet = spreadsheet.get_worksheet(0)
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Basic data cleaning
            # Convert date columns to datetime if needed
            if "Ng√†y SX" in df.columns:
                try:
                    df["Ng√†y SX"] = pd.to_datetime(df["Ng√†y SX"], format="%d/%m/%Y", errors='coerce')
                    df["Production_Month"] = df["Ng√†y SX"].dt.strftime("%m/%Y")
                    df["Production_Date"] = df["Ng√†y SX"]
                except Exception as e:
                    pass
            
            # Make sure numeric columns are properly typed
            if "SL pack/ c√¢y l·ªói" in df.columns:
                df["SL pack/ c√¢y l·ªói"] = pd.to_numeric(df["SL pack/ c√¢y l·ªói"], errors='coerce').fillna(0)
            
            # Ensure Line column is converted to string for consistent filtering
            if "Line" in df.columns:
                df["Line"] = df["Line"].astype(str)
            
            # Ensure M√°y column is converted to string
            if "M√°y" in df.columns:
                df["M√°y"] = df["M√°y"].astype(str)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy c·∫≠p b·∫£ng d·ªØ li·ªáu khi·∫øu n·∫°i: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu khi·∫øu n·∫°i: {str(e)}")
        return pd.DataFrame()

# Function to load AQL data
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the ID AQL worksheet
            try:
                worksheet = spreadsheet.worksheet('ID AQL')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'ID AQL'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Basic data cleaning
            # Convert date columns to datetime if needed
            if "Ng√†y SX" in df.columns:
                try:
                    df["Ng√†y SX"] = pd.to_datetime(df["Ng√†y SX"], format="%d/%m/%Y", errors='coerce')
                    df["Production_Month"] = df["Ng√†y SX"].dt.strftime("%m/%Y")
                    df["Production_Date"] = df["Ng√†y SX"]
                except Exception as e:
                    pass
            
            # Make sure numeric columns are properly typed
            if "S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)" in df.columns:
                df["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"] = pd.to_numeric(df["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"], errors='coerce').fillna(0)
            
            # Ensure Line column is converted to string for consistent filtering
            if "Line" in df.columns:
                df["Line"] = df["Line"].astype(str)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy c·∫≠p b·∫£ng d·ªØ li·ªáu AQL: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu AQL: {str(e)}")
        return pd.DataFrame()

# Function to load AQL g√≥i data
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_goi_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the AQL g√≥i worksheet
            try:
                worksheet = spreadsheet.worksheet('AQL g√≥i')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'AQL g√≥i'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy c·∫≠p b·∫£ng AQL g√≥i: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu AQL g√≥i: {str(e)}")
        return pd.DataFrame()

# Function to load AQL T√¥ ly data
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_aql_to_ly_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the AQL T√¥ ly worksheet
            try:
                worksheet = spreadsheet.worksheet('AQL T√¥ ly')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'AQL T√¥ ly'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy c·∫≠p b·∫£ng AQL T√¥ ly: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu AQL T√¥ ly: {str(e)}")
        return pd.DataFrame()

# Function to load production data (S·∫£n l∆∞·ª£ng)
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_production_data():
    try:
        # Authenticate and connect to Google Sheets
        gc = authenticate()
        
        if gc is None:
            st.error("‚ùå Kh√¥ng th·ªÉ x√°c th·ª±c v·ªõi Google Sheets")
            return pd.DataFrame()
        
        # Open the Google Sheet by URL (AQL data - same spreadsheet, different worksheet)
        sheet_url = "https://docs.google.com/spreadsheets/d/1MxvsyZTMMO0L5Cf1FzuXoKD634OClCCefeLjv9B49XU/edit"
        sheet_key = sheet_url.split('/d/')[1].split('/')[0]
        
        # Open the spreadsheet
        try:
            spreadsheet = gc.open_by_key(sheet_key)
            connection_status = st.empty()
            
            # Get the S·∫£n l∆∞·ª£ng worksheet
            try:
                worksheet = spreadsheet.worksheet('S·∫£n l∆∞·ª£ng')
            except gspread.exceptions.WorksheetNotFound:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng 'S·∫£n l∆∞·ª£ng'")
                return pd.DataFrame()
            
            # Get all records
            data = worksheet.get_all_records()
            
            # Convert to DataFrame
            df = pd.DataFrame(data)
            
            # Basic data cleaning
            # Convert date columns to datetime if needed
            if "Ng√†y" in df.columns:
                try:
                    df["Ng√†y"] = pd.to_datetime(df["Ng√†y"], format="%d/%m/%Y", errors='coerce')
                    df["Production_Month"] = df["Ng√†y"].dt.strftime("%m/%Y")
                    df["Production_Date"] = df["Ng√†y"]
                except Exception as e:
                    pass
            
            # Make sure numeric columns are properly typed
            if "S·∫£n l∆∞·ª£ng" in df.columns:
                df["S·∫£n l∆∞·ª£ng"] = pd.to_numeric(df["S·∫£n l∆∞·ª£ng"], errors='coerce').fillna(0)
            
            # Ensure Line column is converted to string for consistent filtering
            if "Line" in df.columns:
                df["Line"] = df["Line"].astype(str)
            
            # Hide connection status after successful load
            connection_status.empty()
            
            return df
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy c·∫≠p b·∫£ng S·∫£n l∆∞·ª£ng: {str(e)}")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu s·∫£n l∆∞·ª£ng: {str(e)}")
        return pd.DataFrame()

# Function to calculate TEM V√ÄNG - FIXED VERSION
def calculate_tem_vang(aql_df, production_df):
    """Calculate TEM V√ÄNG by matching production data with AQL data"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or production_df.empty:
            st.error("‚ùå Kh√¥ng th·ªÉ t√≠nh TEM V√ÄNG - thi·∫øu d·ªØ li·ªáu")
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        prod_copy = production_df.copy()
        
        # Define function to map time to shift
        def map_time_to_shift(time_str):
            try:
                # Extract hour from time string (format expected: "HH:MM")
                if pd.isna(time_str):
                    return None
                
                # Handle different time formats
                if isinstance(time_str, str):
                    if ':' in time_str:
                        hour = int(time_str.split(':')[0])
                    else:
                        try:
                            hour = int(time_str)
                        except:
                            return None
                elif isinstance(time_str, (int, float)):
                    hour = int(time_str)
                else:
                    return None
                
                # Map hour to shift
                if 6 <= hour < 14:
                    return "1"
                elif 14 <= hour < 22:
                    return "2"
                else:  # 22-6
                    return "3"
            except:
                return None
        
        # Add shift column to AQL data based on Gi·ªù
        if "Gi·ªù" in aql_copy.columns:
            aql_copy["Shift"] = aql_copy["Gi·ªù"].apply(map_time_to_shift)
        else:
            st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt 'Gi·ªù' trong d·ªØ li·ªáu AQL ƒë·ªÉ √°nh x·∫° ca l√†m vi·ªác")
            return pd.DataFrame()
        
        # Ensure all line values are strings
        if "Line" in aql_copy.columns:
            aql_copy["Line"] = aql_copy["Line"].astype(str)
        
        if "Line" in prod_copy.columns:
            prod_copy["Line"] = prod_copy["Line"].astype(str)
        
        # Clean and standardize leader name fields for better matching
        if "T√™n Tr∆∞·ªüng ca" in aql_copy.columns:
            aql_copy["T√™n Tr∆∞·ªüng ca"] = aql_copy["T√™n Tr∆∞·ªüng ca"].astype(str).str.strip()
            
        if "Ng∆∞·ªùi ph·ª• tr√°ch" in prod_copy.columns:
            prod_copy["Ng∆∞·ªùi ph·ª• tr√°ch"] = prod_copy["Ng∆∞·ªùi ph·ª• tr√°ch"].astype(str).str.strip()
            
        # Direct matching approach - process row by row instead of groupby
        tem_vang_data = []
        
        # Process each production record individually to find matches
        for idx, prod_row in prod_copy.iterrows():
            prod_date = prod_row["Ng√†y"]
            prod_line = str(prod_row["Line"])
            prod_volume = prod_row["S·∫£n l∆∞·ª£ng"]
            
            # Standardize shift format
            if isinstance(prod_row["Ca"], (int, float)):
                prod_shift = str(int(prod_row["Ca"]))
            else:
                prod_shift = str(prod_row["Ca"]).strip()
                
            prod_leader = str(prod_row["Ng∆∞·ªùi ph·ª• tr√°ch"]).strip()
            
            # Find matching AQL records - more flexible matching for leader
            matching_records = aql_copy[
                (aql_copy["Ng√†y SX"] == prod_date) &
                (aql_copy["Line"] == prod_line) &
                (aql_copy["Shift"] == prod_shift)
            ]
            
            # Try exact match for leader first
            leader_matched = matching_records[matching_records["T√™n Tr∆∞·ªüng ca"] == prod_leader]
            
            # If no match, try more flexible approach
            if len(leader_matched) == 0:
                # Try finding if leader name is a substring in either direction
                for _, aql_row in matching_records.iterrows():
                    aql_leader = str(aql_row["T√™n Tr∆∞·ªüng ca"]).strip()
                    if (aql_leader in prod_leader) or (prod_leader in aql_leader):
                        leader_matched = pd.concat([leader_matched, aql_row.to_frame().T])
            
            # If still no match, use all matching records by other criteria
            if len(leader_matched) == 0:
                leader_matched = matching_records
            
            # Calculate hold quantity and TEM V√ÄNG
            total_hold = leader_matched["S·ªë l∆∞·ª£ng hold ( g√≥i/th√πng)"].sum() if not leader_matched.empty else 0
            
            if prod_volume > 0:
                tem_vang_percent = (total_hold / prod_volume) * 100
                
                tem_vang_data.append({
                    "Date": prod_date,
                    "Line": prod_line,
                    "Shift": prod_shift,
                    "Leader": prod_leader,
                    "Production_Volume": prod_volume,
                    "Hold_Quantity": total_hold,
                    "TEM_VANG": tem_vang_percent,
                    "Production_Month": prod_date.strftime("%m/%Y") if isinstance(prod_date, datetime) else pd.to_datetime(prod_date).strftime("%m/%Y")
                })
        
        # Convert to DataFrame
        tem_vang_df = pd.DataFrame(tem_vang_data)
        
        # Ensure Production_Date is properly set for filtering
        if not tem_vang_df.empty:
            tem_vang_df["Production_Date"] = tem_vang_df["Date"]
            return tem_vang_df
        else:
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p ƒë·ªÉ t√≠nh TEM V√ÄNG")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t√≠nh TEM V√ÄNG: {str(e)}")
        import traceback
        st.error(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
        return pd.DataFrame()

# Function to analyze defect patterns - FIXED VERSION
def analyze_defect_patterns(aql_df, aql_goi_df, aql_to_ly_df):
    """Analyze defect patterns in AQL data with proper defect name mapping"""
    try:
        # Check if dataframe is empty
        if aql_df.empty:
            return {}
        
        # Create copy to avoid modifying original
        df = aql_df.copy()
        
        # Create defect mapping dictionaries
        defect_goi_map = {}
        defect_to_ly_map = {}
        
        # Build mapping from AQL g√≥i
        if not aql_goi_df.empty and "Defect code" in aql_goi_df.columns and "Defect Name" in aql_goi_df.columns:
            for _, row in aql_goi_df.iterrows():
                key = f"{row['Defect code']}-{row['Type']}" if "Type" in aql_goi_df.columns else row["Defect code"]
                defect_goi_map[key] = row["Defect Name"]
        
        # Build mapping from AQL T√¥ ly
        if not aql_to_ly_df.empty and "Defect code" in aql_to_ly_df.columns and "Defect Name" in aql_to_ly_df.columns:
            for _, row in aql_to_ly_df.iterrows():
                key = f"{row['Defect code']}-{row['Type']}" if "Type" in aql_to_ly_df.columns else row["Defect code"]
                defect_to_ly_map[key] = row["Defect Name"]
        
        # Add Defect Name column to df
        df["Defect_Name"] = None
        
        # Map defect names based on line
        for i, row in df.iterrows():
            line = row["Line"] if "Line" in df.columns else None
            defect_code = row["Defect code"] if "Defect code" in df.columns else None
            defect_type = row["Type"] if "Type" in df.columns else None
            
            if line is not None and defect_code is not None:
                key = f"{defect_code}-{defect_type}" if defect_type is not None else defect_code
                
                # Modified logic to handle non-numeric Line values
                try:
                    # Try to convert line to int for comparison
                    line_int = int(line) if line and not pd.isna(line) else 0
                    # Lines 1-6 use AQL g√≥i
                    if line_int <= 6:
                        df.at[i, "Defect_Name"] = defect_goi_map.get(key, defect_code)
                    # Lines 7-8 use AQL T√¥ ly
                    else:
                        df.at[i, "Defect_Name"] = defect_to_ly_map.get(key, defect_code)
                except (ValueError, TypeError):
                    # For non-numeric lines, default to AQL g√≥i
                    df.at[i, "Defect_Name"] = defect_goi_map.get(key, defect_code)
        
        # Use Defect_Name for analysis if available, otherwise use code
        defect_col = "Defect_Name" if df["Defect_Name"].notna().any() else "Defect code"
        
        # Group by defect to get frequency
        defect_counts = df.groupby(defect_col).size().reset_index(name="Count")
        defect_counts = defect_counts.sort_values("Count", ascending=False)
        
        # Calculate percentages
        total_defects = defect_counts["Count"].sum()
        defect_counts["Percentage"] = (defect_counts["Count"] / total_defects * 100).round(1)
        defect_counts["Cumulative"] = defect_counts["Percentage"].cumsum()
        
        # Identify top defects (80% by Pareto principle)
        vital_few = defect_counts[defect_counts["Cumulative"] <= 80]
        
        # Group by Line and Defect for line-specific patterns
        line_defects = df.groupby(["Line", defect_col]).size().reset_index(name="Count")
        pivot_line_defects = line_defects.pivot(index="Line", columns=defect_col, values="Count").fillna(0)
        
        # Calculate defect rates by MDG
        mdg_defects = pd.DataFrame()
        if "M√°y" in df.columns:
            mdg_defects = df.groupby(["Line", "M√°y", defect_col]).size().reset_index(name="Count")
        
        # Return the analysis results
        return {
            "defect_counts": defect_counts,
            "vital_few": vital_few,
            "line_defects": line_defects,
            "pivot_line_defects": pivot_line_defects,
            "mdg_defects": mdg_defects,
            "defect_column": defect_col
        }
            
    except Exception as e:
        st.error(f"‚ùå L·ªói khi ph√¢n t√≠ch m·∫´u l·ªói: {str(e)}")
        import traceback
        st.error(f"Chi ti·∫øt l·ªói: {traceback.format_exc()}")
        return {}
# Function to analyze defect patterns - UPDATED with defect name mapping
def analyze_defect_patterns(aql_df, aql_goi_df, aql_to_ly_df):
    """Analyze defect patterns in AQL data with proper defect name mapping"""
    try:
        # Check if dataframe is empty
        if aql_df.empty:
            return {}
        
        # Create copy to avoid modifying original
        df = aql_df.copy()
        
        # Create defect mapping dictionaries
        defect_goi_map = {}
        defect_to_ly_map = {}
        
        # Build mapping from AQL g√≥i
        if not aql_goi_df.empty and "Defect code" in aql_goi_df.columns and "Defect Name" in aql_goi_df.columns:
            for _, row in aql_goi_df.iterrows():
                key = f"{row['Defect code']}-{row['Type']}" if "Type" in aql_goi_df.columns else row["Defect code"]
                defect_goi_map[key] = row["Defect Name"]
        
        # Build mapping from AQL T√¥ ly
        if not aql_to_ly_df.empty and "Defect code" in aql_to_ly_df.columns and "Defect Name" in aql_to_ly_df.columns:
            for _, row in aql_to_ly_df.iterrows():
                key = f"{row['Defect code']}-{row['Type']}" if "Type" in aql_to_ly_df.columns else row["Defect code"]
                defect_to_ly_map[key] = row["Defect Name"]
        
        # Add Defect Name column to df
        df["Defect_Name"] = None
        
        # Map defect names based on line
        for i, row in df.iterrows():
            line = row["Line"] if "Line" in df.columns else None
            defect_code = row["Defect code"] if "Defect code" in df.columns else None
            defect_type = row["Type"] if "Type" in df.columns else None
            
            if line is not None and defect_code is not None:
                key = f"{defect_code}-{defect_type}" if defect_type is not None else defect_code
                
                # Lines 1-6 use AQL g√≥i
                if pd.notna(line) and int(line) <= 6:
                    df.at[i, "Defect_Name"] = defect_goi_map.get(key, defect_code)
                # Lines 7-8 use AQL T√¥ ly
                elif pd.notna(line) and int(line) >= 7:
                    df.at[i, "Defect_Name"] = defect_to_ly_map.get(key, defect_code)
        
        # Use Defect_Name for analysis if available, otherwise use code
        defect_col = "Defect_Name" if df["Defect_Name"].notna().any() else "Defect code"
        
        # Group by defect to get frequency
        defect_counts = df.groupby(defect_col).size().reset_index(name="Count")
        defect_counts = defect_counts.sort_values("Count", ascending=False)
        
        # Calculate percentages
        total_defects = defect_counts["Count"].sum()
        defect_counts["Percentage"] = (defect_counts["Count"] / total_defects * 100).round(1)
        defect_counts["Cumulative"] = defect_counts["Percentage"].cumsum()
        
        # Identify top defects (80% by Pareto principle)
        vital_few = defect_counts[defect_counts["Cumulative"] <= 80]
        
        # Group by Line and Defect for line-specific patterns
        line_defects = df.groupby(["Line", defect_col]).size().reset_index(name="Count")
        pivot_line_defects = line_defects.pivot(index="Line", columns=defect_col, values="Count").fillna(0)
        
        # Calculate defect rates by MDG (fixed to use M√°y)
        if "M√°y" in df.columns:
            mdg_defects = df.groupby(["Line", "M√°y", defect_col]).size().reset_index(name="Count")
        else:
            mdg_defects = pd.DataFrame()
        
        # Return the analysis results
        return {
            "defect_counts": defect_counts,
            "vital_few": vital_few,
            "line_defects": line_defects,
            "pivot_line_defects": pivot_line_defects,
            "mdg_defects": mdg_defects,
            "defect_column": defect_col
        }
            
    except Exception as e:
        st.error(f"‚ùå L·ªói khi ph√¢n t√≠ch m·∫´u l·ªói: {str(e)}")
        return {}

# Function to link internal defects with customer complaints
def link_defects_with_complaints(aql_df, complaint_df):
    """Link internal defects (AQL) with customer complaints"""
    try:
        # Check if dataframes are empty
        if aql_df.empty or complaint_df.empty:
            return pd.DataFrame()
        
        # Create copies to avoid modifying originals
        aql_copy = aql_df.copy()
        complaint_copy = complaint_df.copy()
        
        # Standardize date columns for joining
        if "Production_Date" in aql_copy.columns and "Production_Date" in complaint_copy.columns:
            # Group AQL defects by date, line, and defect code
            if "Defect code" in aql_copy.columns and "Line" in aql_copy.columns:
                aql_grouped = aql_copy.groupby(["Production_Date", "Line", "Defect code"]).size().reset_index(name="Defect_Count")
            else:
                st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt c·∫ßn thi·∫øt trong d·ªØ li·ªáu AQL ƒë·ªÉ li√™n k·∫øt")
                return pd.DataFrame()
            
            # Group complaints by date, line, and defect type
            if "T√™n l·ªói" in complaint_copy.columns and "Line" in complaint_copy.columns:
                # Count unique ticket IDs for each group
                if "M√£ ticket" in complaint_copy.columns:
                    complaint_grouped = complaint_copy.groupby(["Production_Date", "Line", "T√™n l·ªói"])["M√£ ticket"].nunique().reset_index(name="Complaint_Count")
                else:
                    complaint_grouped = complaint_copy.groupby(["Production_Date", "Line", "T√™n l·ªói"]).size().reset_index(name="Complaint_Count")
            else:
                st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt c·∫ßn thi·∫øt trong d·ªØ li·ªáu khi·∫øu n·∫°i ƒë·ªÉ li√™n k·∫øt")
                return pd.DataFrame()
            
            # Create mapping between internal defect codes and customer complaint types
            # This mapping should be customized based on your specific defect codes and complaint types
            defect_map = {
                # Example mapping - update with your actual codes
                "NQ-133": "H·ªü n·∫Øp",
                "NQ-124": "R√°ch OPP",
                "HE-022": "M·∫•t date",
                "HE-023": "Thi·∫øu gia v·ªã",
                "NE-023": "H·ªü n·∫Øp",
                "KK-032": "D·ªã v·∫≠t"
            }
            
            # Add mapped complaint type to AQL data
            aql_grouped["Mapped_Complaint_Type"] = aql_grouped["Defect code"].map(defect_map)
            
            # Group AQL data by date, line, and mapped complaint type
            aql_grouped_mapped = aql_grouped.groupby(["Production_Date", "Line", "Mapped_Complaint_Type"])["Defect_Count"].sum().reset_index()
            
            # Rename complaint type column for joining
            complaint_grouped_renamed = complaint_grouped.rename(columns={"T√™n l·ªói": "Mapped_Complaint_Type"})
            
            # Join AQL and complaint data
            # Use a window of +/- 7 days to account for lag between production and complaint
            linked_defects = pd.DataFrame()
            
            for _, aql_row in aql_grouped_mapped.iterrows():
                prod_date = aql_row["Production_Date"]
                line = aql_row["Line"]
                complaint_type = aql_row["Mapped_Complaint_Type"]
                
                # Skip if complaint type mapping is null
                if pd.isna(complaint_type):
                    continue
                
                # Find complaints within the window
                date_min = prod_date - timedelta(days=1)  # 1 day before production
                date_max = prod_date + timedelta(days=14)  # Up to 14 days after production
                
                matching_complaints = complaint_grouped_renamed[
                    (complaint_grouped_renamed["Production_Date"] >= date_min) &
                    (complaint_grouped_renamed["Production_Date"] <= date_max) &
                    (complaint_grouped_renamed["Line"] == line) &
                    (complaint_grouped_renamed["Mapped_Complaint_Type"] == complaint_type)
                ]
                
                if not matching_complaints.empty:
                    total_complaints = matching_complaints["Complaint_Count"].sum()
                    
                    linked_row = pd.DataFrame({
                        "Production_Date": [prod_date],
                        "Line": [line],
                        "Defect_Type": [complaint_type],
                        "Internal_Defect_Count": [aql_row["Defect_Count"]],
                        "Customer_Complaint_Count": [total_complaints],
                        "Defect_to_Complaint_Ratio": [aql_row["Defect_Count"] / total_complaints if total_complaints > 0 else float('inf')]
                    })
                    
                    linked_defects = pd.concat([linked_defects, linked_row], ignore_index=True)
            
            return linked_defects
            
        else:
            st.warning("‚ö†Ô∏è Thi·∫øu c·ªôt ng√†y ƒë·ªÉ li√™n k·∫øt l·ªói v·ªõi khi·∫øu n·∫°i")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå L·ªói khi li√™n k·∫øt l·ªói v·ªõi khi·∫øu n·∫°i: {str(e)}")
        return pd.DataFrame()

# Load the data - UPDATED to include new data sources
@st.cache_data(ttl=600)  # Cache the combined data for 10 minutes
def load_all_data():
    """Load and prepare all required data"""
    
    # Load raw data
    complaint_df = load_complaint_data()
    aql_df = load_aql_data()
    production_df = load_production_data()
    
    # Load defect mapping data
    aql_goi_df = load_aql_goi_data()
    aql_to_ly_df = load_aql_to_ly_data()
    
    # Calculate TEM V√ÄNG
    tem_vang_df = calculate_tem_vang(aql_df, production_df)
    
    # Analyze defect patterns - UPDATED to use defect mapping
    defect_patterns = analyze_defect_patterns(aql_df, aql_goi_df, aql_to_ly_df)
    
    # Link defects with complaints
    linked_defects_df = link_defects_with_complaints(aql_df, complaint_df)
    
    return {
        "complaint_data": complaint_df,
        "aql_data": aql_df,
        "production_data": production_df,
        "aql_goi_data": aql_goi_df,
        "aql_to_ly_data": aql_to_ly_df,
        "tem_vang_data": tem_vang_df,
        "defect_patterns": defect_patterns,
        "linked_defects": linked_defects_df
    }

# Title and description
st.markdown('<div class="main-header">B√°o c√°o ch·∫•t l∆∞·ª£ng CF MMB</div>', unsafe_allow_html=True)
st.markdown("B√°o c√°o t·ªïng h·ª£p v·ªÅ ch·∫•t l∆∞·ª£ng s·∫£n xu·∫•t v√† m·ª©c ƒë·ªô h√†i l√≤ng c·ªßa kh√°ch h√†ng")

# Load all data
data = load_all_data()

# Check if key dataframes are empty
if data["aql_data"].empty or data["production_data"].empty:
    st.warning("‚ö†Ô∏è D·ªØ li·ªáu ch∆∞a ƒë·∫ßy ƒë·ªß. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn Google Sheet.")
    # Still continue rendering with available data

# Create a sidebar for filters
with st.sidebar:
    st.markdown("<h2 style='text-align: center; color: #1E3A8A;'>B·ªô l·ªçc</h2>", unsafe_allow_html=True)
    
    # Initialize filtered dataframes
    filtered_aql_df = data["aql_data"].copy()
    filtered_complaint_df = data["complaint_data"].copy()
    filtered_tem_vang_df = data["tem_vang_data"].copy()
    
    # NEW - Date range filter
    if not data["aql_data"].empty and "Production_Date" in data["aql_data"].columns:
        try:
            # Get min and max dates from data
            min_date = data["aql_data"]["Production_Date"].min().date()
            max_date = data["aql_data"]["Production_Date"].max().date()
            
            st.subheader("üìÖ Ph·∫°m vi ng√†y")
            # Create date range selector
            col1, col2 = st.columns(2)
            with col1:
                start_date = st.date_input("Ng√†y b·∫Øt ƒë·∫ßu", min_date, min_value=min_date, max_value=max_date)
            with col2:
                end_date = st.date_input("Ng√†y k·∫øt th√∫c", max_date, min_value=min_date, max_value=max_date)
            
            # Apply date filter to dataframes
            if "Production_Date" in filtered_aql_df.columns:
                filtered_aql_df = filtered_aql_df[
                    (filtered_aql_df["Production_Date"].dt.date >= start_date) & 
                    (filtered_aql_df["Production_Date"].dt.date <= end_date)
                ]
            
            if "Production_Date" in filtered_complaint_df.columns:
                filtered_complaint_df = filtered_complaint_df[
                    (filtered_complaint_df["Production_Date"].dt.date >= start_date) & 
                    (filtered_complaint_df["Production_Date"].dt.date <= end_date)
                ]
            
            if "Production_Date" in filtered_tem_vang_df.columns:
                filtered_tem_vang_df = filtered_tem_vang_df[
                    (filtered_tem_vang_df["Production_Date"].dt.date >= start_date) & 
                    (filtered_tem_vang_df["Production_Date"].dt.date <= end_date)
                ]
        except Exception as e:
            st.warning(f"L·ªói khi l·ªçc theo ng√†y: {e}")
    
    # Line filter
    if not data["tem_vang_data"].empty and "Line" in data["tem_vang_data"].columns:
        try:
            lines = ["T·∫•t c·∫£"] + sorted(data["tem_vang_data"]["Line"].unique().tolist())
            selected_line = st.selectbox("üè≠ Ch·ªçn line s·∫£n xu·∫•t", lines)
            
            if selected_line != "T·∫•t c·∫£":
                filtered_tem_vang_df = filtered_tem_vang_df[filtered_tem_vang_df["Line"] == selected_line]
                
                # Apply to other dataframes
                if "Line" in filtered_aql_df.columns:
                    filtered_aql_df = filtered_aql_df[filtered_aql_df["Line"] == selected_line]
                
                if "Line" in filtered_complaint_df.columns:
                    filtered_complaint_df = filtered_complaint_df[filtered_complaint_df["Line"] == selected_line]
        except Exception as e:
            st.warning(f"L·ªói khi l·ªçc theo line: {e}")
    
    # Product filter
    if not data["complaint_data"].empty and "T√™n s·∫£n ph·∫©m" in data["complaint_data"].columns:
        try:
            products = ["T·∫•t c·∫£"] + sorted(data["complaint_data"]["T√™n s·∫£n ph·∫©m"].unique().tolist())
            selected_product = st.selectbox("üçú Ch·ªçn s·∫£n ph·∫©m", products)
            
            if selected_product != "T·∫•t c·∫£":
                filtered_complaint_df = filtered_complaint_df[filtered_complaint_df["T√™n s·∫£n ph·∫©m"] == selected_product]
                
                # Filter AQL data by item if possible
                if "T√™n s·∫£n ph·∫©m" in filtered_aql_df.columns:
                    filtered_aql_df = filtered_aql_df[filtered_aql_df["T√™n s·∫£n ph·∫©m"] == selected_product]
        except Exception as e:
            st.warning(f"L·ªói khi l·ªçc theo s·∫£n ph·∫©m: {e}")
    
    # Refresh button
    if st.button("üîÑ L√†m m·ªõi d·ªØ li·ªáu", use_container_width=True):
        st.cache_data.clear()
        st.experimental_rerun()
    
    # Show last update time
    st.markdown(f"**C·∫≠p nh·∫≠t g·∫ßn nh·∫•t:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
    
    # Add auto-refresh checkbox
    auto_refresh = st.checkbox("‚è±Ô∏è T·ª± ƒë·ªông l√†m m·ªõi (5p)", value=False)

# Main dashboard layout with tabs for the 3 pages
tab1, tab2, tab3 = st.tabs([
    "üìà Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng s·∫£n xu·∫•t", 
    "üîç Ph√¢n t√≠ch khi·∫øu n·∫°i kh√°ch h√†ng",
    "üîÑ Li√™n k·∫øt ch·∫•t l∆∞·ª£ng n·ªôi b·ªô - b√™n ngo√†i"
])

# Page 1: Production Quality Analysis (TEM V√ÄNG and defects by line/MDG)
with tab1:
    st.markdown('<div class="sub-header">T·ªïng quan ch·∫•t l∆∞·ª£ng s·∫£n xu·∫•t</div>', unsafe_allow_html=True)
    
    # Key metrics row
    metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
    
    with metrics_col1:
        if not filtered_tem_vang_df.empty:
            avg_tem_vang = filtered_tem_vang_df["TEM_VANG"].mean()
            tem_target = 2.18  # TEM V√ÄNG target
            tem_delta = avg_tem_vang - tem_target
            
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">TEM V√ÄNG trung b√¨nh</div>
                <div class="metric-value">{avg_tem_vang:.2f}%</div>
                <div style="color: {'red' if tem_delta > 0 else 'green'};">
                    {f"{tem_delta:.2f}% {'cao h∆°n' if tem_delta > 0 else 'th·∫•p h∆°n'} m·ª•c ti√™u"}
                </div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">TEM V√ÄNG trung b√¨nh</div>
                <div class="metric-value">N/A</div>
            </div>
            """, unsafe_allow_html=True)
    
    with metrics_col2:
        if not filtered_tem_vang_df.empty:
            total_hold = filtered_tem_vang_df["Hold_Quantity"].sum()
            
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">T·ªïng s·ªë l∆∞·ª£ng hold</div>
                <div class="metric-value">{total_hold:,.0f}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">T·ªïng s·ªë l∆∞·ª£ng hold</div>
                <div class="metric-value">N/A</div>
            </div>
            """, unsafe_allow_html=True)
    
    with metrics_col3:
        if not filtered_aql_df.empty and "Defect code" in filtered_aql_df.columns:
            defect_types = filtered_aql_df["Defect code"].nunique()
            
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">Lo·∫°i l·ªói</div>
                <div class="metric-value">{defect_types}</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">Lo·∫°i l·ªói</div>
                <div class="metric-value">N/A</div>
            </div>
            """, unsafe_allow_html=True)
    
    # TEM V√ÄNG Analysis
    st.markdown('<div class="sub-header">Ph√¢n t√≠ch TEM V√ÄNG</div>', unsafe_allow_html=True)
    
    tem_col1, tem_col2 = st.columns(2)
    
    with tem_col1:
        # TEM V√ÄNG trend over time
        if not filtered_tem_vang_df.empty:
            try:
                # Group by date to get daily average TEM V√ÄNG
                daily_tem_vang = filtered_tem_vang_df.groupby("Date")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
                
                # Sort by date
                daily_tem_vang = daily_tem_vang.sort_values("Date")
                
                # Create figure
                fig = go.Figure()
                
                # Add TEM V√ÄNG line
                fig.add_trace(go.Scatter(
                    x=daily_tem_vang["Date"],
                    y=daily_tem_vang["TEM_VANG"],
                    mode="lines+markers",
                    name="TEM V√ÄNG",
                    line=dict(color="royalblue", width=2),
                    marker=dict(size=6)
                ))
                
                # Add target line
                fig.add_hline(
                    y=2.18,
                    line_dash="dash",
                    line_color="red",
                    annotation_text="M·ª•c ti√™u (2.18%)"
                )
                
                # Update layout
                fig.update_layout(
                    title="Xu h∆∞·ªõng TEM V√ÄNG theo th·ªùi gian",
                    xaxis_title="Ng√†y",
                    yaxis_title="TEM V√ÄNG (%)",
                    height=350,
                    margin=dict(l=40, r=40, t=40, b=40)
                )
                
                st.plotly_chart(fig, use_container_width=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì xu h∆∞·ªõng TEM V√ÄNG: {str(e)}")
    
    with tem_col2:
        # TEM V√ÄNG by line
        if not filtered_tem_vang_df.empty:
            try:
                # Group by line to get average TEM V√ÄNG per line
                line_tem_vang = filtered_tem_vang_df.groupby("Line")[["TEM_VANG", "Hold_Quantity"]].mean().reset_index()
                
                # Sort by TEM V√ÄNG value
                line_tem_vang = line_tem_vang.sort_values("TEM_VANG", ascending=False)
                
                # Create figure
                fig = go.Figure()
                
                # Add TEM V√ÄNG bars
                fig.add_trace(go.Bar(
                    x=line_tem_vang["Line"],
                    y=line_tem_vang["TEM_VANG"],
                    name="TEM V√ÄNG",
                    marker_color="royalblue",
                    text=line_tem_vang["TEM_VANG"].round(2).astype(str) + "%",
                    textposition="auto"
                ))
                
                # Add target line
                fig.add_hline(
                    y=2.18,
                    line_dash="dash",
                    line_color="red",
                    annotation_text="M·ª•c ti√™u (2.18%)"
                )
                
                # Update layout
                fig.update_layout(
                    title="TEM V√ÄNG theo Line s·∫£n xu·∫•t",
                    xaxis_title="Line",
                    yaxis_title="TEM V√ÄNG (%)",
                    height=350,
                    margin=dict(l=40, r=40, t=40, b=40)
                )
                
                st.plotly_chart(fig, use_container_width=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì TEM V√ÄNG theo line: {str(e)}")
    
    # Defect Analysis by Line and MDG
    st.markdown('<div class="sub-header">Ph√¢n t√≠ch l·ªói theo Line v√† MDG</div>', unsafe_allow_html=True)
    
    defect_col1, defect_col2 = st.columns(2)
    
    with defect_col1:
        # Pareto chart of defects
        if "defect_patterns" in data and "defect_counts" in data["defect_patterns"]:
            try:
                defect_counts = data["defect_patterns"]["defect_counts"]
                defect_col = data["defect_patterns"].get("defect_column", "Defect code")
                
                # Create Pareto chart
                fig = make_subplots(specs=[[{"secondary_y": True}]])
                
                # Add bars for defect counts
                fig.add_trace(
                    go.Bar(
                        x=defect_counts[defect_col],
                        y=defect_counts["Count"],
                        name="S·ªë l∆∞·ª£ng l·ªói",
                        marker_color="steelblue"
                    ),
                    secondary_y=False
                )
                
                # Add line for cumulative percentage
                fig.add_trace(
                    go.Scatter(
                        x=defect_counts[defect_col],
                        y=defect_counts["Cumulative"],
                        name="T√≠ch l≈©y %",
                        mode="lines+markers",
                        marker=dict(color="firebrick"),
                        line=dict(color="firebrick", width=2)
                    ),
                    secondary_y=True
                )
                
                # Add 80% reference line
                fig.add_hline(
                    y=80,
                    line_dash="dash",
                    line_color="green",
                    annotation_text="80% l·ªói",
                    secondary_y=True
                )
                
                # Update layout
                fig.update_layout(
                    title="Ph√¢n t√≠ch Pareto c·ªßa c√°c l·ªói",
                    xaxis_title="Lo·∫°i l·ªói",
                    height=350,
                    margin=dict(l=40, r=40, t=40, b=40)
                )
                
                # Set y-axes titles
                fig.update_yaxes(title_text="S·ªë l∆∞·ª£ng l·ªói", secondary_y=False)
                fig.update_yaxes(title_text="T√≠ch l≈©y %", secondary_y=True)
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Add Pareto analysis insight
                if "vital_few" in data["defect_patterns"]:
                    vital_few = data["defect_patterns"]["vital_few"]
                    
                    st.markdown(f"""
                    <div class="insight-card">
                        <div class="insight-title">Ph√¢n t√≠ch Pareto</div>
                        <div class="insight-content">
                            <p>{len(vital_few)} lo·∫°i l·ªói ({len(vital_few)/len(defect_counts)*100:.0f}% t·ªïng s·ªë lo·∫°i) chi·∫øm 80% t·∫•t c·∫£ c√°c l·ªói.</p>
                            <p>T·∫≠p trung c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng v√†o: {', '.join(vital_few[defect_col].tolist()[:5])}</p>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì Pareto: {str(e)}")
    
    with defect_col2:
        # Defects by line heatmap
        if "defect_patterns" in data and "pivot_line_defects" in data["defect_patterns"]:
            try:
                pivot_df = data["defect_patterns"]["pivot_line_defects"]
                
                if not pivot_df.empty:
                    # Create heatmap
                    fig = px.imshow(
                        pivot_df,
                        labels=dict(x="Lo·∫°i l·ªói", y="Line", color="S·ªë l∆∞·ª£ng"),
                        x=pivot_df.columns,
                        y=pivot_df.index,
                        color_continuous_scale="YlOrRd",
                        aspect="auto"
                    )
                    
                    # Update layout
                    fig.update_layout(
                        title="Ph√¢n b·ªë l·ªói theo Line",
                        height=350,
                        margin=dict(l=40, r=40, t=40, b=40)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu l·ªói ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì nhi·ªát")
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì nhi·ªát l·ªói: {str(e)}")
    
    # MDG Analysis
    st.markdown('<div class="sub-header">Ph√¢n t√≠ch MDG (M√°y)</div>', unsafe_allow_html=True)
    
    if "defect_patterns" in data and "mdg_defects" in data["defect_patterns"] and not data["defect_patterns"]["mdg_defects"].empty:
        try:
            mdg_defects = data["defect_patterns"]["mdg_defects"].copy()
            defect_col = data["defect_patterns"].get("defect_column", "Defect code")
            
            # Group by Line and MDG to get total defects
            line_mdg_summary = mdg_defects.groupby(["Line", "M√°y"])["Count"].sum().reset_index()
            
            # Create bar chart
            fig = px.bar(
                line_mdg_summary,
                x="M√°y",
                y="Count",
                color="Line",
                title="L·ªói theo MDG (M√°y) v√† Line",
                labels={"M√°y": "MDG (M√°y)", "Count": "S·ªë l∆∞·ª£ng l·ªói"},
                barmode="group"
            )
            
            # Update layout
            fig.update_layout(
                height=400,
                margin=dict(l=40, r=40, t=40, b=40)
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Display top MDG-defect combinations
            st.markdown("#### Top c√°c k·∫øt h·ª£p MDG-L·ªói")
            
            # Group by Line, MDG, and Defect code
            top_mdg_defects = mdg_defects.sort_values("Count", ascending=False).head(10)
            
            # Create a styled dataframe
            st.dataframe(top_mdg_defects, use_container_width=True, height=250)
            
        except Exception as e:
            st.error(f"L·ªói trong ph√¢n t√≠ch MDG: {str(e)}")
    else:
        st.warning("‚ö†Ô∏è D·ªØ li·ªáu ph√¢n t√≠ch MDG kh√¥ng c√≥ s·∫µn. Ki·ªÉm tra xem c·ªôt 'M√°y' c√≥ t·ªìn t·∫°i trong b·∫£ng ID AQL kh√¥ng.")

# Page 2: Customer Complaint Analysis
with tab2:
    st.markdown('<div class="sub-header">T·ªïng quan khi·∫øu n·∫°i kh√°ch h√†ng</div>', unsafe_allow_html=True)
    
    # Check if complaint dataframe is empty
    if filtered_complaint_df.empty:
        st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu khi·∫øu n·∫°i kh·∫£ d·ª•ng ƒë·ªÉ ph√¢n t√≠ch")
    else:
        # Add Line filter for complaint data specifically
        if "Line" in filtered_complaint_df.columns:
            complaint_lines = ["T·∫•t c·∫£"] + sorted(filtered_complaint_df["Line"].unique().tolist())
            selected_complaint_line = st.selectbox("üè≠ Ch·ªçn Line s·∫£n xu·∫•t cho ph√¢n t√≠ch khi·∫øu n·∫°i", complaint_lines)
            
            if selected_complaint_line != "T·∫•t c·∫£":
                filtered_complaint_df = filtered_complaint_df[filtered_complaint_df["Line"] == selected_complaint_line]
        
        # Key metrics row
        comp_col1, comp_col2, comp_col3, comp_col4 = st.columns(4)
        
        with comp_col1:
            if "M√£ ticket" in filtered_complaint_df.columns:
                total_complaints = filtered_complaint_df["M√£ ticket"].nunique()
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-title">T·ªïng khi·∫øu n·∫°i</div>
                    <div class="metric-value">{total_complaints}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("Thi·∫øu c·ªôt 'M√£ ticket'")
        
        with comp_col2:
            if "SL pack/ c√¢y l·ªói" in filtered_complaint_df.columns:
                total_defective_packs = filtered_complaint_df["SL pack/ c√¢y l·ªói"].sum()
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-title">G√≥i l·ªói</div>
                    <div class="metric-value">{total_defective_packs:,.0f}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("Thi·∫øu c·ªôt 'SL pack/ c√¢y l·ªói'")
        
        with comp_col3:
            if "T·ªânh" in filtered_complaint_df.columns:
                total_provinces = filtered_complaint_df["T·ªânh"].nunique()
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-title">T·ªânh b·ªã ·∫£nh h∆∞·ªüng</div>
                    <div class="metric-value">{total_provinces}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("Thi·∫øu c·ªôt 'T·ªânh'")
        
        with comp_col4:
            if "T√™n l·ªói" in filtered_complaint_df.columns:
                total_defect_types = filtered_complaint_df["T√™n l·ªói"].nunique()
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-title">Lo·∫°i l·ªói</div>
                    <div class="metric-value">{total_defect_types}</div>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("Thi·∫øu c·ªôt 'T√™n l·ªói'")
        
        # Complaint Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch khi·∫øu n·∫°i</div>', unsafe_allow_html=True)
        
        comp_col1, comp_col2 = st.columns(2)
        
        with comp_col1:
            if "T√™n s·∫£n ph·∫©m" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by product
                    product_complaints = filtered_complaint_df.groupby("T√™n s·∫£n ph·∫©m").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Sort by complaint count
                    product_complaints = product_complaints.sort_values("M√£ ticket", ascending=False).head(10)
                    
                    # Create figure - IMPROVED VISUALIZATION
                    fig = go.Figure()
                    
                    # Add bars for complaints - horizontal for better readability of product names
                    fig.add_trace(go.Bar(
                        y=product_complaints["T√™n s·∫£n ph·∫©m"],
                        x=product_complaints["M√£ ticket"],
                        name="Khi·∫øu n·∫°i",
                        orientation='h',
                        marker_color='firebrick',
                        text=product_complaints["M√£ ticket"],
                        textposition="outside"
                    ))
                    
                    # Update layout
                    fig.update_layout(
                        title="Top 10 s·∫£n ph·∫©m b·ªã khi·∫øu n·∫°i",
                        xaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        yaxis_title="S·∫£n ph·∫©m",
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=40)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì khi·∫øu n·∫°i theo s·∫£n ph·∫©m: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt c·∫ßn thi·∫øt cho bi·ªÉu ƒë·ªì s·∫£n ph·∫©m")
        
        with comp_col2:
            # IMPROVED - Changed to horizontal bar chart for better readability
            if "T√™n l·ªói" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by defect type
                    defect_complaints = filtered_complaint_df.groupby("T√™n l·ªói").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Calculate percentages
                    defect_complaints["Complaint %"] = (defect_complaints["M√£ ticket"] / defect_complaints["M√£ ticket"].sum() * 100).round(1)
                    
                    # Sort by count for better visualization
                    defect_complaints = defect_complaints.sort_values("M√£ ticket", ascending=False)
                    
                    # Create horizontal bar chart
                    fig = go.Figure()
                    
                    # Add horizontal bars
                    fig.add_trace(go.Bar(
                        y=defect_complaints["T√™n l·ªói"],
                        x=defect_complaints["M√£ ticket"],
                        orientation='h',
                        marker_color='firebrick',
                        text=defect_complaints["Complaint %"].astype(str) + "%",
                        textposition="auto"
                    ))
                    
                    # Update layout
                    fig.update_layout(
                        title="Khi·∫øu n·∫°i theo lo·∫°i l·ªói",
                        xaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        yaxis_title="Lo·∫°i l·ªói",
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=40)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì khi·∫øu n·∫°i theo lo·∫°i l·ªói: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt c·∫ßn thi·∫øt cho bi·ªÉu ƒë·ªì l·ªói")
        
        # Complaint Timeline and Production Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch d√≤ng th·ªùi gian khi·∫øu n·∫°i</div>', unsafe_allow_html=True)
        
        time_col1, time_col2 = st.columns(2)
        
        with time_col1:
            if "Production_Date" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by date
                    date_complaints = filtered_complaint_df.groupby("Production_Date").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Sort by date
                    date_complaints = date_complaints.sort_values("Production_Date")
                    
                    # Create figure - CHANGED TO COLUMNS instead of line for better visualization
                    fig = go.Figure()
                    
                    # Add column bars for complaints
                    fig.add_trace(go.Bar(
                        x=date_complaints["Production_Date"],
                        y=date_complaints["M√£ ticket"],
                        name="Khi·∫øu n·∫°i",
                        marker_color="royalblue"
                    ))
                    
                    # Update layout
                    fig.update_layout(
                        title="Xu h∆∞·ªõng khi·∫øu n·∫°i theo th·ªùi gian",
                        xaxis_title="Ng√†y s·∫£n xu·∫•t",
                        yaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=40)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì d√≤ng th·ªùi gian khi·∫øu n·∫°i: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt ng√†y cho bi·ªÉu ƒë·ªì d√≤ng th·ªùi gian")
        
        with time_col2:
            if "Line" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by line
                    line_complaints = filtered_complaint_df.groupby("Line").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Sort by complaint count
                    line_complaints = line_complaints.sort_values("M√£ ticket", ascending=False)
                    
                    # Create figure
                    fig = go.Figure()
                    
                    # Add bars for complaints
                    fig.add_trace(go.Bar(
                        x=line_complaints["Line"],
                        y=line_complaints["M√£ ticket"],
                        name="Khi·∫øu n·∫°i",
                        marker_color="navy",
                        text=line_complaints["M√£ ticket"],
                        textposition="outside"
                    ))
                    
                    # Add a secondary y-axis for defective packs
                    fig.add_trace(go.Scatter(
                        x=line_complaints["Line"],
                        y=line_complaints["SL pack/ c√¢y l·ªói"],
                        name="S·ªë g√≥i l·ªói",
                        mode="markers",
                        marker=dict(size=12, color="firebrick"),
                        yaxis="y2"
                    ))
                    
                    # Update layout with secondary y-axis
                    # IMPROVED - Restrict x-axis to lines 1-8 only
                    fig.update_layout(
                        title="Khi·∫øu n·∫°i theo Line s·∫£n xu·∫•t",
                        xaxis_title="Line s·∫£n xu·∫•t",
                        yaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        yaxis2=dict(
                            title="S·ªë g√≥i l·ªói",
                            anchor="x",
                            overlaying="y",
                            side="right"
                        ),
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=40),
                        legend=dict(orientation="h", yanchor="bottom", y=1.02),
                        xaxis=dict(
                            tickmode='array',
                            tickvals=list(range(1, 9)),
                            ticktext=[str(i) for i in range(1, 9)]
                        )
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì khi·∫øu n·∫°i theo line: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt Line cho bi·ªÉu ƒë·ªì line")
        
        # Geographic Distribution of Complaints
        st.markdown('<div class="sub-header">Ph√¢n b·ªë ƒë·ªãa l√Ω</div>', unsafe_allow_html=True)
        
        if "T·ªânh" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
            try:
                # Group by province
                province_complaints = filtered_complaint_df.groupby("T·ªânh").agg({
                    "M√£ ticket": "nunique",
                    "SL pack/ c√¢y l·ªói": "sum"
                }).reset_index()
                
                # Sort by complaint count
                province_complaints = province_complaints.sort_values("M√£ ticket", ascending=False)
                
                # Create figure
                fig = px.bar(
                    province_complaints.head(15),  # Top 15 provinces
                    x="T·ªânh",
                    y="M√£ ticket",
                    color="SL pack/ c√¢y l·ªói",
                    title="Top t·ªânh theo s·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                    labels={"T·ªânh": "T·ªânh", "M√£ ticket": "S·ªë l∆∞·ª£ng khi·∫øu n·∫°i", "SL pack/ c√¢y l·ªói": "G√≥i l·ªói"},
                    color_continuous_scale="Viridis"
                )
                
                # Update layout
                fig.update_layout(
                    height=400,
                    margin=dict(l=40, r=40, t=40, b=100),
                    xaxis_tickangle=-45
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Calculate percentages for top provinces
                top_provinces = province_complaints.head(5)
                total_complaints = province_complaints["M√£ ticket"].sum()
                top_provinces["Percentage"] = (top_provinces["M√£ ticket"] / total_complaints * 100).round(1)
                
                # Display insight
                st.markdown(f"""
                <div class="insight-card">
                    <div class="insight-title">Th√¥ng tin ƒë·ªãa l√Ω</div>
                    <div class="insight-content">
                        <p>Top 5 t·ªânh chi·∫øm {top_provinces['Percentage'].sum():.1f}% t·ªïng s·ªë khi·∫øu n·∫°i.</p>
                        <p>T·ªânh cao nh·∫•t ({top_provinces.iloc[0]['T·ªânh']}) chi·∫øm {top_provinces.iloc[0]['Percentage']:.1f}% t·ªïng s·ªë khi·∫øu n·∫°i.</p>
                        <p>Xem x√©t c√°c ch∆∞∆°ng tr√¨nh n√¢ng cao ch·∫•t l∆∞·ª£ng c√≥ m·ª•c ti√™u ·ªü c√°c khu v·ª±c c√≥ khi·∫øu n·∫°i cao n√†y.</p>
                    </div>
                </div>
                """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì ph√¢n b·ªë ƒë·ªãa l√Ω: {str(e)}")
        else:
            st.warning("Thi·∫øu c·ªôt t·ªânh cho ph√¢n t√≠ch ƒë·ªãa l√Ω")
        
        # Personnel Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch nh√¢n s·ª± s·∫£n xu·∫•t</div>', unsafe_allow_html=True)
        
        personnel_col1, personnel_col2 = st.columns(2)
        
        with personnel_col1:
            if "QA" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by QA
                    qa_complaints = filtered_complaint_df.groupby("QA").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Remove NaN values
                    qa_complaints = qa_complaints.dropna(subset=["QA"])
                    
                    # Sort by complaint count
                    qa_complaints = qa_complaints.sort_values("M√£ ticket", ascending=False)
                    
                    # Create figure
                    fig = go.Figure()
                    
                    # Add bars for complaints
                    fig.add_trace(go.Bar(
                        x=qa_complaints["QA"],
                        y=qa_complaints["M√£ ticket"],
                        name="Khi·∫øu n·∫°i",
                        marker_color="purple",
                        text=qa_complaints["M√£ ticket"],
                        textposition="outside"
                    ))
                    
                    # Update layout
                    fig.update_layout(
                        title="Khi·∫øu n·∫°i theo nh√¢n vi√™n QA",
                        xaxis_title="Nh√¢n vi√™n QA",
                        yaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=80),
                        xaxis_tickangle=-45
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì khi·∫øu n·∫°i theo QA: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt QA cho ph√¢n t√≠ch nh√¢n s·ª±")
        
        with personnel_col2:
            if "T√™n Tr∆∞·ªüng ca" in filtered_complaint_df.columns and "M√£ ticket" in filtered_complaint_df.columns:
                try:
                    # Group by shift leader
                    leader_complaints = filtered_complaint_df.groupby("T√™n Tr∆∞·ªüng ca").agg({
                        "M√£ ticket": "nunique",
                        "SL pack/ c√¢y l·ªói": "sum"
                    }).reset_index()
                    
                    # Remove NaN values
                    leader_complaints = leader_complaints.dropna(subset=["T√™n Tr∆∞·ªüng ca"])
                    
                    # Sort by complaint count
                    leader_complaints = leader_complaints.sort_values("M√£ ticket", ascending=False)
                    
                    # Create figure
                    fig = go.Figure()
                    
                    # Add bars for complaints
                    fig.add_trace(go.Bar(
                        x=leader_complaints["T√™n Tr∆∞·ªüng ca"],
                        y=leader_complaints["M√£ ticket"],
                        name="Khi·∫øu n·∫°i",
                        marker_color="darkred",
                        text=leader_complaints["M√£ ticket"],
                        textposition="outside"
                    ))
                    
                    # Update layout
                    fig.update_layout(
                        title="Khi·∫øu n·∫°i theo tr∆∞·ªüng ca",
                        xaxis_title="Tr∆∞·ªüng ca",
                        yaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i",
                        height=400,
                        margin=dict(l=40, r=40, t=40, b=80),
                        xaxis_tickangle=-45
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì khi·∫øu n·∫°i theo tr∆∞·ªüng ca: {str(e)}")
            else:
                st.warning("Thi·∫øu c·ªôt tr∆∞·ªüng ca cho ph√¢n t√≠ch nh√¢n s·ª±")
        
        # Complaint Details Table
        st.markdown('<div class="sub-header">Chi ti·∫øt khi·∫øu n·∫°i</div>', unsafe_allow_html=True)
        
        try:
            # Create a display dataframe with key columns
            if not filtered_complaint_df.empty:
                display_cols = [
                    "M√£ ticket", "Ng√†y ti·∫øp nh·∫≠n", "T·ªânh", "Ng√†y SX", "T√™n s·∫£n ph·∫©m",
                    "SL pack/ c√¢y l·ªói", "T√™n l·ªói", "Line", "QA", "T√™n Tr∆∞·ªüng ca"
                ]
                
                # Only include columns that exist in the dataframe
                display_cols = [col for col in display_cols if col in filtered_complaint_df.columns]
                
                # Create display dataframe
                display_df = filtered_complaint_df[display_cols].copy()
                
                # Sort by most recent complaints first
                if "Ng√†y ti·∫øp nh·∫≠n" in display_df.columns:
                    display_df = display_df.sort_values("Ng√†y ti·∫øp nh·∫≠n", ascending=False)
                
                # Format dates for display
                for date_col in ["Ng√†y ti·∫øp nh·∫≠n", "Ng√†y SX"]:
                    if date_col in display_df.columns and pd.api.types.is_datetime64_any_dtype(display_df[date_col]):
                        display_df[date_col] = display_df[date_col].dt.strftime("%d/%m/%Y")
                
                # Display the table
                st.dataframe(display_df, use_container_width=True, height=400)
            else:
                st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu khi·∫øu n·∫°i ƒë·ªÉ hi·ªÉn th·ªã")
        except Exception as e:
            st.error(f"L·ªói khi hi·ªÉn th·ªã chi ti·∫øt khi·∫øu n·∫°i: {str(e)}")

# Page 3: Linking Internal and External Quality
with tab3:
    st.markdown('<div class="sub-header">Ph√¢n t√≠ch li√™n k·∫øt ch·∫•t l∆∞·ª£ng n·ªôi b·ªô - b√™n ngo√†i</div>', unsafe_allow_html=True)
    
    # Check if linked defects data is available
    if "linked_defects" in data and not data["linked_defects"].empty:
        # Key metrics row
        link_col1, link_col2, link_col3, link_col4 = st.columns(4)
        
        linked_df = data["linked_defects"].copy()
        
        with link_col1:
            total_linkages = len(linked_df)
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">T·ªïng s·ªë li√™n k·∫øt</div>
                <div class="metric-value">{total_linkages}</div>
            </div>
            """, unsafe_allow_html=True)
        
        with link_col2:
            avg_ratio = linked_df["Defect_to_Complaint_Ratio"].replace([float('inf'), -float('inf')], np.nan).mean()
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">T·ª∑ l·ªá L·ªói:Khi·∫øu n·∫°i TB</div>
                <div class="metric-value">{avg_ratio:.1f}</div>
            </div>
            """, unsafe_allow_html=True)
        
        with link_col3:
            unique_defect_types = linked_df["Defect_Type"].nunique()
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">Lo·∫°i l·ªói li√™n k·∫øt</div>
                <div class="metric-value">{unique_defect_types}</div>
            </div>
            """, unsafe_allow_html=True)
        
        with link_col4:
            total_lines = linked_df["Line"].nunique()
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-title">Line b·ªã ·∫£nh h∆∞·ªüng</div>
                <div class="metric-value">{total_lines}</div>
            </div>
            """, unsafe_allow_html=True)
        
        # Defect to Complaint Ratio Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch t·ª∑ l·ªá L·ªói-Khi·∫øu n·∫°i</div>', unsafe_allow_html=True)
        
        ratio_col1, ratio_col2 = st.columns(2)
        
        with ratio_col1:
            try:
                # Group by defect type
                defect_type_ratios = linked_df.groupby("Defect_Type").agg({
                    "Internal_Defect_Count": "sum",
                    "Customer_Complaint_Count": "sum"
                }).reset_index()
                
                # Calculate overall ratio
                defect_type_ratios["Ratio"] = defect_type_ratios["Internal_Defect_Count"] / defect_type_ratios["Customer_Complaint_Count"]
                
                # Sort by ratio
                defect_type_ratios = defect_type_ratios.sort_values("Ratio")
                
                # Create figure
                fig = go.Figure()
                
                # Add bars for ratio
                fig.add_trace(go.Bar(
                    y=defect_type_ratios["Defect_Type"],
                    x=defect_type_ratios["Ratio"],
                    orientation="h",
                    marker_color="teal",
                    text=defect_type_ratios["Ratio"].round(1),
                    textposition="outside"
                ))
                
                # Update layout
                fig.update_layout(
                    title="T·ª∑ l·ªá L·ªói-Khi·∫øu n·∫°i theo lo·∫°i l·ªói",
                    xaxis_title="T·ª∑ l·ªá (L·ªói n·ªôi b·ªô : Khi·∫øu n·∫°i kh√°ch h√†ng)",
                    yaxis_title="Lo·∫°i l·ªói",
                    height=400,
                    margin=dict(l=40, r=40, t=40, b=40)
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Add interpretation
                st.markdown(f"""
                <div class="insight-card">
                    <div class="insight-title">Di·ªÖn gi·∫£i t·ª∑ l·ªá</div>
                    <div class="insight-content">
                        <p>T·ª∑ l·ªá cao h∆°n cho bi·∫øt nhi·ªÅu l·ªói n·ªôi b·ªô ƒë∆∞·ª£c ph√°t hi·ªán cho m·ªói khi·∫øu n·∫°i c·ªßa kh√°ch h√†ng.</p>
                        <p>T·ª∑ l·ªá th·∫•p h∆°n cho th·∫•y r·∫±ng l·ªói kh√¥ng ƒë∆∞·ª£c ph√°t hi·ªán hi·ªáu qu·∫£ trong qu√° tr√¨nh s·∫£n xu·∫•t.</p>
                        <p><strong>{defect_type_ratios.iloc[-1]['Defect_Type']}</strong> c√≥ t·ª∑ l·ªá cao nh·∫•t ({defect_type_ratios.iloc[-1]['Ratio']:.1f}), cho th·∫•y hi·ªáu qu·∫£ ph√°t hi·ªán n·ªôi b·ªô.</p>
                        <p><strong>{defect_type_ratios.iloc[0]['Defect_Type']}</strong> c√≥ t·ª∑ l·ªá th·∫•p nh·∫•t ({defect_type_ratios.iloc[0]['Ratio']:.1f}), cho th·∫•y c·∫ßn c·∫£i thi·ªán ph√°t hi·ªán.</p>
                    </div>
                </div>
                """, unsafe_allow_html=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t·ª∑ l·ªá: {str(e)}")
        
        with ratio_col2:
            try:
                # Group by line
                line_ratios = linked_df.groupby("Line").agg({
                    "Internal_Defect_Count": "sum",
                    "Customer_Complaint_Count": "sum"
                }).reset_index()
                
                # Calculate overall ratio
                line_ratios["Ratio"] = line_ratios["Internal_Defect_Count"] / line_ratios["Customer_Complaint_Count"]
                
                # Create scatter plot
                fig = px.scatter(
                    line_ratios,
                    x="Internal_Defect_Count",
                    y="Customer_Complaint_Count",
                    size="Ratio",
                    color="Line",
                    hover_name="Line",
                    text="Line",
                    title="L·ªói n·ªôi b·ªô & Khi·∫øu n·∫°i kh√°ch h√†ng theo Line"
                )
                
                # Update markers
                fig.update_traces(
                    marker=dict(sizemode="area", sizeref=0.1),
                    textposition="top center"
                )
                
                # Add diagonal reference line (1:1 ratio)
                max_val = max(line_ratios["Internal_Defect_Count"].max(), line_ratios["Customer_Complaint_Count"].max())
                fig.add_trace(go.Scatter(
                    x=[0, max_val],
                    y=[0, max_val],
                    mode="lines",
                    line=dict(color="gray", dash="dash"),
                    name="T·ª∑ l·ªá 1:1"
                ))
                
                # Update layout
                fig.update_layout(
                    xaxis_title="S·ªë l∆∞·ª£ng l·ªói n·ªôi b·ªô",
                    yaxis_title="S·ªë l∆∞·ª£ng khi·∫øu n·∫°i kh√°ch h√†ng",
                    height=400,
                    margin=dict(l=40, r=40, t=40, b=40)
                )
                
                st.plotly_chart(fig, use_container_width=True)
            except Exception as e:
                st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì t·ª∑ l·ªá theo line: {str(e)}")
        
        # Timeline Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch theo th·ªùi gian</div>', unsafe_allow_html=True)
        
        try:
            # Group by date
            date_analysis = linked_df.groupby("Production_Date").agg({
                "Internal_Defect_Count": "sum",
                "Customer_Complaint_Count": "sum"
            }).reset_index()
            
            # Calculate ratio
            date_analysis["Ratio"] = date_analysis["Internal_Defect_Count"] / date_analysis["Customer_Complaint_Count"]
            
            # Sort by date
            date_analysis = date_analysis.sort_values("Production_Date")
            
            # Create figure with secondary y-axis
            fig = make_subplots(specs=[[{"secondary_y": True}]])
            
            # Add lines for internal defects and customer complaints
            fig.add_trace(
                go.Scatter(
                    x=date_analysis["Production_Date"],
                    y=date_analysis["Internal_Defect_Count"],
                    name="L·ªói n·ªôi b·ªô",
                    mode="lines+markers",
                    line=dict(color="royalblue", width=2)
                ),
                secondary_y=False
            )
            
            fig.add_trace(
                go.Scatter(
                    x=date_analysis["Production_Date"],
                    y=date_analysis["Customer_Complaint_Count"],
                    name="Khi·∫øu n·∫°i kh√°ch h√†ng",
                    mode="lines+markers",
                    line=dict(color="firebrick", width=2)
                ),
                secondary_y=False
            )
            
            # Add ratio line
            fig.add_trace(
                go.Scatter(
                    x=date_analysis["Production_Date"],
                    y=date_analysis["Ratio"],
                    name="T·ª∑ l·ªá L·ªói:Khi·∫øu n·∫°i",
                    mode="lines",
                    line=dict(color="green", width=2, dash="dash")
                ),
                secondary_y=True
            )
            
            # Update layout
            fig.update_layout(
                title="L·ªói n·ªôi b·ªô v√† khi·∫øu n·∫°i kh√°ch h√†ng theo th·ªùi gian",
                xaxis_title="Ng√†y s·∫£n xu·∫•t",
                height=400,
                margin=dict(l=40, r=40, t=40, b=40),
                legend=dict(orientation="h", yanchor="bottom", y=1.02)
            )
            
            # Set y-axes titles
            fig.update_yaxes(title_text="S·ªë l∆∞·ª£ng", secondary_y=False)
            fig.update_yaxes(title_text="T·ª∑ l·ªá L·ªói:Khi·∫øu n·∫°i", secondary_y=True)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Calculate correlation
            correlation = date_analysis["Internal_Defect_Count"].corr(date_analysis["Customer_Complaint_Count"])
            
            # Add insight about correlation
            st.markdown(f"""
            <div class="insight-card">
                <div class="insight-title">Ph√¢n t√≠ch t∆∞∆°ng quan</div>
                <div class="insight-content">
                    <p>T∆∞∆°ng quan gi·ªØa l·ªói n·ªôi b·ªô v√† khi·∫øu n·∫°i kh√°ch h√†ng l√† <strong>{correlation:.2f}</strong>.</p>
                    <p>{'T∆∞∆°ng quan d∆∞∆°ng n√†y cho th·∫•y tƒÉng l·ªói n·ªôi b·ªô c√≥ li√™n quan ƒë·∫øn tƒÉng khi·∫øu n·∫°i c·ªßa kh√°ch h√†ng, v·ªõi ƒë·ªô tr·ªÖ t·ª´ v√†i ng√†y ƒë·∫øn v√†i tu·∫ßn.' if correlation > 0 else 'T∆∞∆°ng quan n√†y cho th·∫•y l·ªói n·ªôi b·ªô v√† khi·∫øu n·∫°i c·ªßa kh√°ch h√†ng c√≥ th·ªÉ kh√¥ng tr·ª±c ti·∫øp li√™n quan ho·∫∑c c√≥ ƒë·ªô tr·ªÖ ƒë√°ng k·ªÉ gi·ªØa v·∫•n ƒë·ªÅ s·∫£n xu·∫•t v√† ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng.'}</p>
                </div>
            </div>
            """, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch theo th·ªùi gian: {str(e)}")
        
        # Detection Effectiveness Analysis
        st.markdown('<div class="sub-header">Ph√¢n t√≠ch hi·ªáu qu·∫£ ph√°t hi·ªán</div>', unsafe_allow_html=True)
        
        try:
            # Calculate detection effectiveness for each defect type
            effectiveness_df = linked_df.groupby("Defect_Type").agg({
                "Internal_Defect_Count": "sum",
                "Customer_Complaint_Count": "sum"
            }).reset_index()
            
            # Calculate effectiveness percentage
            effectiveness_df["Total_Issues"] = effectiveness_df["Internal_Defect_Count"] + effectiveness_df["Customer_Complaint_Count"]
            effectiveness_df["Detection_Effectiveness"] = (effectiveness_df["Internal_Defect_Count"] / effectiveness_df["Total_Issues"] * 100).round(1)
            
            # Sort by effectiveness
            effectiveness_df = effectiveness_df.sort_values("Detection_Effectiveness")
            
            # Create figure
            fig = go.Figure()
            
            # Add bars for effectiveness
            fig.add_trace(go.Bar(
                y=effectiveness_df["Defect_Type"],
                x=effectiveness_df["Detection_Effectiveness"],
                orientation="h",
                marker_color=effectiveness_df["Detection_Effectiveness"].map(lambda x: "green" if x >= 90 else ("orange" if x >= 75 else "red")),
                text=effectiveness_df["Detection_Effectiveness"].astype(str) + "%",
                textposition="outside"
            ))
            
            # Add reference lines
            fig.add_vline(x=75, line_dash="dash", line_color="orange", annotation_text="75% (Ch·∫•p nh·∫≠n ƒë∆∞·ª£c)")
            fig.add_vline(x=90, line_dash="dash", line_color="green", annotation_text="90% (Xu·∫•t s·∫Øc)")
            
            # Update layout
            fig.update_layout(
                title="Hi·ªáu qu·∫£ ph√°t hi·ªán ch·∫•t l∆∞·ª£ng n·ªôi b·ªô theo lo·∫°i l·ªói",
                xaxis_title="Hi·ªáu qu·∫£ ph√°t hi·ªán (%)",
                yaxis_title="Lo·∫°i l·ªói",
                height=400,
                margin=dict(l=40, r=40, t=40, b=40),
                xaxis=dict(range=[0, 100])
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Identify poor detection areas
            poor_detection = effectiveness_df[effectiveness_df["Detection_Effectiveness"] < 75]
            
            if not poor_detection.empty:
                st.markdown(f"""
                <div class="warning-card">
                    <div class="warning-title">Khu v·ª±c ph√°t hi·ªán k√©m</div>
                    <div class="insight-content">
                        <p>C√°c lo·∫°i l·ªói sau c√≥ hi·ªáu qu·∫£ ph√°t hi·ªán d∆∞·ªõi 75%, cho th·∫•y c∆° h·ªôi c·∫£i thi·ªán ƒë√°ng k·ªÉ:</p>
                        <ul>
                            {''.join([f"<li><strong>{row['Defect_Type']}</strong>: {row['Detection_Effectiveness']}% hi·ªáu qu·∫£</li>" for _, row in poor_detection.iterrows()])}
                        </ul>
                        <p>Xem x√©t th·ª±c hi·ªán c·∫£i ti·∫øn c√≥ m·ª•c ti√™u trong ph∆∞∆°ng ph√°p ph√°t hi·ªán c√°c lo·∫°i l·ªói n√†y.</p>
                    </div>
                </div>
                """, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o ph√¢n t√≠ch hi·ªáu qu·∫£ ph√°t hi·ªán: {str(e)}")
    else:
        st.warning("""
        ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu l·ªói li√™n k·∫øt. ƒêi·ªÅu n√†y c√≥ th·ªÉ do:
        
        1. D·ªØ li·ªáu l·ªãch s·ª≠ kh√¥ng ƒë·ªß ƒë·ªÉ thi·∫øt l·∫≠p k·∫øt n·ªëi
        2. M√£ l·ªói kh√¥ng kh·ªõp gi·ªØa d·ªØ li·ªáu n·ªôi b·ªô v√† kh√°ch h√†ng
        3. V·∫•n ƒë·ªÅ t√≠ch h·ª£p d·ªØ li·ªáu
        
        Vui l√≤ng ƒë·∫£m b·∫£o c·∫£ d·ªØ li·ªáu AQL v√† khi·∫øu n·∫°i ƒë·ªÅu c√≥ s·∫µn v√† ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng ƒë√∫ng.
        """)

# Footer with dashboard information
st.markdown("""
<div style="text-align: center; padding: 15px; margin-top: 30px; border-top: 1px solid #eee;">
    <p style="color: #555; font-size: 0.9rem;">
        B√°o c√°o ch·∫•t l∆∞·ª£ng CF MMB | T·∫°o b·ªüi Ph√≤ng ƒê·∫£m b·∫£o Ch·∫•t l∆∞·ª£ng
    </p>
</div>
""", unsafe_allow_html=True)

# Auto-refresh mechanism
if auto_refresh:
    time.sleep(300)  # Wait for 5 minutes
    st.experimental_rerun()
