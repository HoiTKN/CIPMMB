import os
import sys
import io
import requests
import pandas as pd
import matplotlib.pyplot as plt
import smtplib
import msal
import base64
import traceback
import time
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders

# Global processor variable
global_processor = None

# SharePoint Configuration
SHAREPOINT_CONFIG = {
    'tenant_id': '81060475-7e7f-4ede-8d8d-bf61f53ca528',
    'client_id': '076541aa-c734-405e-8518-ed52b67f8cbd',
    'authority': 'https://login.microsoftonline.com/81060475-7e7f-4ede-8d8d-bf61f53ca528',
    'scopes': ['https://graph.microsoft.com/Sites.ReadWrite.All'],
    'site_name': 'MCH.MMB.QA',
    'base_url': 'masangroup.sharepoint.com'
}

# SharePoint File ID for "CIP plan.xlsx"
CIP_PLAN_FILE_ID = '8C90FB38-DA8C-59CC-547D-53BEA1C8B16D'

class GitHubSecretsUpdater:
    """Helper class to update GitHub Secrets using GitHub API"""
    def __init__(self, repo_owner, repo_name, github_token):
        self.repo_owner = repo_owner
        self.repo_name = repo_name
        self.github_token = github_token
        self.api_base = "https://api.github.com"
    
    def get_public_key(self):
        """Get repository public key for encrypting secrets"""
        url = f"{self.api_base}/repos/{self.repo_owner}/{self.repo_name}/actions/secrets/public-key"
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"Failed to get public key: {response.status_code}")
    
    def encrypt_secret(self, public_key, secret_value):
        """Encrypt secret using repository public key"""
        from nacl import encoding, public
        
        public_key_obj = public.PublicKey(public_key.encode("utf-8"), encoding.Base64Encoder())
        sealed_box = public.SealedBox(public_key_obj)
        encrypted = sealed_box.encrypt(secret_value.encode("utf-8"))
        
        return base64.b64encode(encrypted).decode("utf-8")
    
    def update_secret(self, secret_name, secret_value):
        """Update a GitHub secret"""
        try:
            # Get public key
            key_data = self.get_public_key()
            
            # Encrypt secret
            encrypted_value = self.encrypt_secret(key_data["key"], secret_value)
            
            # Update secret
            url = f"{self.api_base}/repos/{self.repo_owner}/{self.repo_name}/actions/secrets/{secret_name}"
            headers = {
                "Authorization": f"token {self.github_token}",
                "Accept": "application/vnd.github.v3+json"
            }
            data = {
                "encrypted_value": encrypted_value,
                "key_id": key_data["key_id"]
            }
            
            response = requests.put(url, headers=headers, json=data)
            if response.status_code in [201, 204]:
                print(f"‚úÖ Successfully updated {secret_name}")
                return True
            else:
                print(f"‚ùå Failed to update {secret_name}: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Error updating secret: {str(e)}")
            return False

class SharePointCIPProcessor:
    """SharePoint integration for CIP Cleaning automation"""
    
    def __init__(self):
        self.access_token = None
        self.refresh_token = None
        self.base_url = "https://graph.microsoft.com/v1.0"
        self.site_id = None
        self.msal_app = None
        
        # Initialize MSAL app
        self.msal_app = msal.PublicClientApplication(
            SHAREPOINT_CONFIG['client_id'],
            authority=SHAREPOINT_CONFIG['authority']
        )
        
        # Authenticate on initialization
        if not self.authenticate():
            raise Exception("SharePoint authentication failed during initialization")

    def log(self, message):
        """Log with timestamp"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] {message}")
        sys.stdout.flush()

    def authenticate(self):
        """Authenticate using delegation flow with pre-generated tokens"""
        try:
            self.log("üîê Authenticating with SharePoint...")

            # Get tokens from environment variables
            access_token = os.environ.get('SHAREPOINT_ACCESS_TOKEN')
            refresh_token = os.environ.get('SHAREPOINT_REFRESH_TOKEN')

            if not access_token and not refresh_token:
                self.log("‚ùå No SharePoint tokens found in environment variables")
                return False

            self.access_token = access_token
            self.refresh_token = refresh_token
            
            if access_token:
                self.log(f"‚úÖ Found access token: {access_token[:30]}...")
                
                # Test token validity
                if self.test_token_validity():
                    self.log("‚úÖ SharePoint access token is valid")
                    return True
                else:
                    self.log("‚ö†Ô∏è SharePoint access token expired, attempting refresh...")
                    
            # Try to refresh token
            if refresh_token:
                if self.refresh_access_token():
                    self.log("‚úÖ SharePoint token refreshed successfully")
                    self.update_github_secrets()
                    return True
                else:
                    self.log("‚ùå SharePoint token refresh failed")
                    return False
            else:
                self.log("‚ùå No SharePoint refresh token available")
                return False

        except Exception as e:
            self.log(f"‚ùå SharePoint authentication error: {str(e)}")
            return False

    def test_token_validity(self):
        """Test if current access token is valid"""
        try:
            headers = self.get_headers()
            response = requests.get(f"{self.base_url}/me", headers=headers, timeout=30)

            if response.status_code == 200:
                user_info = response.json()
                self.log(f"‚úÖ Authenticated to SharePoint as: {user_info.get('displayName', 'Unknown')}")
                return True
            elif response.status_code == 401:
                return False
            else:
                self.log(f"Warning: Unexpected response code: {response.status_code}")
                return False

        except Exception as e:
            self.log(f"Error testing SharePoint token validity: {str(e)}")
            return False

    def refresh_access_token(self):
        """Refresh access token using refresh token with MSAL"""
        try:
            if not self.refresh_token:
                self.log("‚ùå No refresh token available")
                return False

            self.log("üîÑ Attempting to refresh SharePoint token using MSAL...")

            # Use MSAL to refresh token
            result = self.msal_app.acquire_token_by_refresh_token(
                self.refresh_token,
                scopes=SHAREPOINT_CONFIG['scopes']
            )

            if result and "access_token" in result:
                self.access_token = result['access_token']
                if 'refresh_token' in result:
                    self.refresh_token = result['refresh_token']
                    self.log("‚úÖ Got new refresh token")
                
                self.log("‚úÖ SharePoint token refreshed successfully")
                return True
            else:
                error = result.get('error_description', 'Unknown error') if result else 'No result'
                self.log(f"‚ùå SharePoint token refresh failed: {error}")
                return False

        except Exception as e:
            self.log(f"‚ùå Error refreshing SharePoint token: {str(e)}")
            return False

    def update_github_secrets(self):
        """Update GitHub Secrets with new tokens"""
        try:
            github_token = os.environ.get('GITHUB_TOKEN')
            if not github_token:
                self.log("‚ö†Ô∏è No GITHUB_TOKEN found, cannot update secrets")
                return False
            
            repo = os.environ.get('GITHUB_REPOSITORY', '')
            if '/' not in repo:
                self.log("‚ö†Ô∏è Invalid GITHUB_REPOSITORY format")
                return False
            
            repo_owner, repo_name = repo.split('/')
            updater = GitHubSecretsUpdater(repo_owner, repo_name, github_token)
            
            # Update access token
            if self.access_token:
                updater.update_secret('SHAREPOINT_ACCESS_TOKEN', self.access_token)
            
            # Update refresh token
            if self.refresh_token:
                updater.update_secret('SHAREPOINT_REFRESH_TOKEN', self.refresh_token)
            
            return True
            
        except Exception as e:
            self.log(f"‚ö†Ô∏è Error updating GitHub Secrets: {str(e)}")
            return False

    def get_headers(self):
        """Get headers for API requests"""
        return {
            'Authorization': f'Bearer {self.access_token}',
            'Content-Type': 'application/json'
        }

    def get_site_id(self):
        """Get SharePoint site ID"""
        try:
            if self.site_id:
                return self.site_id

            url = f"{self.base_url}/sites/{SHAREPOINT_CONFIG['base_url']}:/sites/{SHAREPOINT_CONFIG['site_name']}"
            response = requests.get(url, headers=self.get_headers(), timeout=30)

            if response.status_code == 200:
                site_data = response.json()
                self.site_id = site_data['id']
                self.log(f"‚úÖ Found SharePoint site ID: {self.site_id}")
                return self.site_id
            else:
                self.log(f"‚ùå Error getting SharePoint site ID: {response.status_code}")
                return None

        except Exception as e:
            self.log(f"‚ùå Error getting SharePoint site ID: {str(e)}")
            return None

    def download_excel_file(self):
        """Download Excel file from SharePoint"""
        try:
            self.log(f"üì• Downloading CIP plan file from SharePoint...")

            # Get file download URL using file ID
            url = f"{self.base_url}/sites/{self.get_site_id()}/drive/items/{CIP_PLAN_FILE_ID}"
            response = requests.get(url, headers=self.get_headers(), timeout=30)

            if response.status_code == 200:
                file_info = response.json()
                download_url = file_info.get('@microsoft.graph.downloadUrl')

                if download_url:
                    # Download file content
                    self.log(f"‚úÖ Got download URL, downloading content...")
                    file_response = requests.get(download_url, timeout=60)

                    if file_response.status_code == 200:
                        # Read Excel from memory
                        excel_data = io.BytesIO(file_response.content)
                        self.log(f"‚úÖ Downloaded {len(file_response.content)} bytes")
                        
                        try:
                            excel_file = pd.ExcelFile(excel_data)
                            sheets_data = {}
                            
                            self.log(f"Excel sheets found: {excel_file.sheet_names}")
                            
                            for sheet_name in excel_file.sheet_names:
                                excel_data.seek(0)
                                df = pd.read_excel(excel_data, sheet_name=sheet_name)
                                sheets_data[sheet_name] = df
                                self.log(f"‚úÖ Sheet '{sheet_name}': {len(df)} rows, {len(df.columns)} columns")
                            
                            self.log(f"‚úÖ Successfully downloaded CIP plan file")
                            return sheets_data
                            
                        except Exception as e:
                            self.log(f"‚ùå Error reading Excel file: {str(e)}")
                            return None
                    else:
                        self.log(f"‚ùå Error downloading file content: {file_response.status_code}")
                else:
                    self.log(f"‚ùå No download URL found for CIP plan file")
            else:
                self.log(f"‚ùå Error getting file info: {response.status_code}")

        except Exception as e:
            self.log(f"‚ùå Error downloading CIP plan file: {str(e)}")

        return None

    def upload_excel_file(self, sheets_data):
        """Upload updated Excel file back to SharePoint with retry logic for locked files"""
        max_retries = 5
        retry_delay = 30  # seconds
        
        try:
            self.log(f"üì§ Uploading updated CIP plan to SharePoint...")

            # Create Excel file in memory with multiple sheets
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                for sheet_name, df in sheets_data.items():
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

            excel_buffer.seek(0)
            excel_content = excel_buffer.getvalue()
            self.log(f"Created Excel file with {len(excel_content)} bytes")

            # Upload to SharePoint with retry logic
            upload_url = f"{self.base_url}/sites/{self.get_site_id()}/drive/items/{CIP_PLAN_FILE_ID}/content"

            headers = {
                'Authorization': f'Bearer {self.access_token}',
                'Content-Type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            }

            for attempt in range(max_retries):
                try:
                    self.log(f"Upload attempt {attempt + 1}/{max_retries}")
                    
                    response = requests.put(upload_url, headers=headers, data=excel_content, timeout=60)

                    if response.status_code in [200, 201]:
                        self.log(f"‚úÖ Successfully uploaded updated CIP plan to SharePoint")
                        return True
                    elif response.status_code == 423:
                        # File is locked
                        self.log(f"‚ö†Ô∏è File is locked (attempt {attempt + 1}/{max_retries})")
                        if attempt < max_retries - 1:
                            self.log(f"‚è≥ Waiting {retry_delay} seconds before retry...")
                            time.sleep(retry_delay)
                            continue
                        else:
                            self.log(f"‚ùå File remains locked after {max_retries} attempts")
                            # Try to save to a backup location or with different name
                            return self.upload_backup_file(excel_content)
                    elif response.status_code == 401:
                        # Token expired, try refresh
                        self.log("üîÑ Token expired during upload, refreshing...")
                        if self.refresh_access_token():
                            self.update_github_secrets()
                            headers['Authorization'] = f'Bearer {self.access_token}'
                            continue
                        else:
                            self.log("‚ùå Token refresh failed during upload")
                            return False
                    else:
                        self.log(f"‚ùå Error uploading to SharePoint: {response.status_code}")
                        self.log(f"Response: {response.text[:500]}")
                        if attempt < max_retries - 1:
                            self.log(f"‚è≥ Retrying in {retry_delay} seconds...")
                            time.sleep(retry_delay)
                            continue
                        else:
                            return False

                except requests.exceptions.RequestException as e:
                    self.log(f"‚ùå Network error during upload: {str(e)}")
                    if attempt < max_retries - 1:
                        self.log(f"‚è≥ Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        return False

            return False

        except Exception as e:
            self.log(f"‚ùå Error uploading to SharePoint: {str(e)}")
            return False

    def upload_backup_file(self, excel_content):
        """Upload to a backup file when original is locked"""
        try:
            # Generate backup filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_filename = f"CIP_plan_backup_{timestamp}.xlsx"
            
            self.log(f"üîÑ Uploading to backup file: {backup_filename}")
            
            # Upload to the same folder but with different name
            # First get the parent folder
            file_info_url = f"{self.base_url}/sites/{self.get_site_id()}/drive/items/{CIP_PLAN_FILE_ID}"
            response = requests.get(file_info_url, headers=self.get_headers(), timeout=30)
            
            if response.status_code == 200:
                file_info = response.json()
                parent_id = file_info.get('parentReference', {}).get('id')
                
                if parent_id:
                    # Upload to parent folder with new name
                    upload_url = f"{self.base_url}/sites/{self.get_site_id()}/drive/items/{parent_id}:/{backup_filename}:/content"
                    
                    headers = {
                        'Authorization': f'Bearer {self.access_token}',
                        'Content-Type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                    }
                    
                    response = requests.put(upload_url, headers=headers, data=excel_content, timeout=60)
                    
                    if response.status_code in [200, 201]:
                        self.log(f"‚úÖ Successfully uploaded backup file: {backup_filename}")
                        self.log(f"‚ö†Ô∏è Original file was locked, please check and rename backup file manually")
                        return True
                    else:
                        self.log(f"‚ùå Failed to upload backup file: {response.status_code}")
                        return False
                else:
                    self.log(f"‚ùå Could not get parent folder information")
                    return False
            else:
                self.log(f"‚ùå Could not get file information for backup: {response.status_code}")
                return False
                
        except Exception as e:
            self.log(f"‚ùå Error uploading backup file: {str(e)}")
            return False

    def update_sheet_data(self, sheet_name, df):
        """Update specific sheet data in SharePoint Excel file"""
        # For now, we'll update the entire file. 
        # In future, could implement partial sheet updates if needed
        pass

# Helper function to parse dates in different formats
def parse_date(date_str):
    """Try to parse date with multiple formats and handle Excel date formats"""
    if not date_str or str(date_str).strip() in ['nan', 'None', '', 'NaT']:
        return None
    
    # If it's already a datetime object, return it
    if isinstance(date_str, datetime):
        return date_str
    
    # If it's a pandas timestamp, convert it
    if hasattr(date_str, 'to_pydatetime'):
        try:
            return date_str.to_pydatetime()
        except:
            pass
    
    # Convert to string and clean
    date_str = str(date_str).strip()
    
    # Handle Excel serial dates (numbers like 45123.0)
    try:
        # If it's a number that could be an Excel date
        if date_str.replace('.', '').isdigit():
            excel_date = float(date_str)
            # Excel date serial numbers are typically > 1 and < 50000 for reasonable dates
            if 1 < excel_date < 50000:
                # Excel epoch is 1900-01-01 (with some quirks)
                excel_epoch = datetime(1900, 1, 1)
                # Excel incorrectly treats 1900 as a leap year, so subtract 2 days
                return excel_epoch + timedelta(days=excel_date - 2)
    except (ValueError, TypeError):
        pass
    
    # Try various date formats
    date_formats = [
        '%B %d, %Y',     # June 7, 2025
        '%d/%m/%Y',      # 07/06/2025
        '%m/%d/%Y',      # 06/07/2025
        '%Y-%m-%d',      # 2025-06-07
        '%d-%m-%Y',      # 07-06-2025
        '%d %B %Y',      # 7 June 2025
        '%d %B, %Y',     # 7 June, 2025
        '%d/%m/%y',      # 07/06/25
        '%m/%d/%y',      # 06/07/25
        '%d-%m-%y',      # 07-06-25
        '%d.%m.%Y',      # 07.06.2025
        '%d.%m.%y',      # 07.06.25
        '%Y/%m/%d',      # 2025/06/07
        '%d-%b-%Y',      # 07-Jun-2025
        '%d-%b-%y',      # 07-Jun-25
        '%d %b %Y',      # 7 Jun 2025
        '%d %b %y',      # 7 Jun 25
        '%Y-%m-%d %H:%M:%S',  # 2025-06-07 00:00:00
        '%B %d %Y',      # June 7 2025 (no comma)
        '%b %d, %Y',     # Jun 7, 2025
        '%b %d %Y',      # Jun 7 2025
    ]
    
    for fmt in date_formats:
        try:
            parsed_date = datetime.strptime(date_str, fmt)
            # Sanity check - date should be between 1900 and 2100
            if 1900 <= parsed_date.year <= 2100:
                return parsed_date
        except (ValueError, TypeError):
            continue
    
    # Try pandas to_datetime as last resort
    try:
        import pandas as pd
        parsed_date = pd.to_datetime(date_str, dayfirst=True, errors='coerce')
        if not pd.isna(parsed_date):
            return parsed_date.to_pydatetime()
    except:
        pass
    
    print(f"‚ö†Ô∏è Warning: Could not parse date: '{date_str}'")
    return None

# Main function to update cleaning schedule using SharePoint
def update_cleaning_schedule():
    global global_processor  # Use global processor
    
    print("ƒêang c·∫≠p nh·∫≠t l·ªãch v·ªá sinh t·ª´ SharePoint...")
    
    # Initialize SharePoint processor
    global_processor = SharePointCIPProcessor()
    
    # Download Excel file from SharePoint
    sheets_data = global_processor.download_excel_file()
    if not sheets_data:
        print("‚ùå Failed to download CIP plan file")
        return []
    
    # Get or create sheets data
    master_plan_df = sheets_data.get('Master plan', pd.DataFrame())
    cleaning_history_df = sheets_data.get('Cleaning History', pd.DataFrame())
    actual_result_df = sheets_data.get('Actual result', pd.DataFrame())
    
    # Initialize sheets if empty
    if master_plan_df.empty:
        headers = ['Khu v·ª±c', 'Thi·∫øt b·ªã', 'Ph∆∞∆°ng ph√°p', 'T·∫ßn su·∫•t (ng√†y)', 
                'Ng√†y v·ªá sinh g·∫ßn nh·∫•t', 'Ng√†y k·∫ø ho·∫°ch v·ªá sinh ti·∫øp theo', 'Tr·∫°ng th√°i', 'ƒêang ch·ª©a s·∫£n ph·∫©m']
        master_plan_df = pd.DataFrame(columns=headers)
        sheets_data['Master plan'] = master_plan_df
    
    if cleaning_history_df.empty:
        history_headers = ['Khu v·ª±c', 'Thi·∫øt b·ªã', 'Ph∆∞∆°ng ph√°p', 
                        'T·∫ßn su·∫•t (ng√†y)', 'Ng√†y v·ªá sinh', 'Ng∆∞·ªùi th·ª±c hi·ªán']
        cleaning_history_df = pd.DataFrame(columns=history_headers)
        sheets_data['Cleaning History'] = cleaning_history_df
    
    if actual_result_df.empty:
        actual_headers = ['Khu v·ª±c', 'Thi·∫øt b·ªã', 'Ph∆∞∆°ng ph√°p', 'T·∫ßn su·∫•t (ng√†y)', 
                           'Ng√†y v·ªá sinh', 'Ng∆∞·ªùi th·ª±c hi·ªán', 'K·∫øt qu·∫£', 'Ghi ch√∫']
        actual_result_df = pd.DataFrame(columns=actual_headers)
        sheets_data['Actual result'] = actual_result_df
    
    # Process Master plan data
    today = datetime.today()
    updated_values = []
    
    # Initialize counters FIRST - CRITICAL!
    processed_count = 0
    status_counts = {'B√¨nh th∆∞·ªùng': 0, 'S·∫Øp ƒë·∫øn h·∫°n': 0, 'ƒê·∫øn h·∫°n': 0, 'Qu√° h·∫°n': 0, 'Ch∆∞a c√≥ d·ªØ li·ªáu': 0, 'L·ªói': 0}
    
    # Check if data already has status column - maybe we don't need to calculate
    print(f"üîç Checking existing status data...")
    
    # Check if there's already a status column with data
    existing_status_col = None
    for col in master_plan_df.columns:
        col_lower = str(col).lower().strip()
        if 'tr·∫°ng th√°i' in col_lower:
            existing_status_col = col
            break
    
    if existing_status_col:
        print(f"‚úÖ Found existing status column: '{existing_status_col}'")
        existing_statuses = master_plan_df[existing_status_col].value_counts()
        print(f"üìä Existing status breakdown:")
        for status, count in existing_statuses.items():
            print(f"  - '{status}': {count}")
        
        # If we already have status data, let's use it and just update dates
        use_existing_status = True
        print(f"üîÑ Using existing status data instead of recalculating")
    else:
        print(f"‚ö†Ô∏è No existing status column found, will calculate status")
        use_existing_status = False
    
    for idx, row in master_plan_df.iterrows():
        try:
            # Get values, handle missing columns with more flexible column name matching
            area = ''
            device = ''
            method = ''
            freq_str = ''
            last_cleaning = ''
            has_product = ''
            existing_status = ''
            
            # More flexible column matching
            for col in master_plan_df.columns:
                col_lower = str(col).lower().strip()
                if 'khu' in col_lower and 'v·ª±c' in col_lower:
                    area = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 'thi·∫øt' in col_lower and 'b·ªã' in col_lower:
                    device = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 'ph∆∞∆°ng' in col_lower and 'ph√°p' in col_lower:
                    method = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 't·∫ßn' in col_lower and 'su·∫•t' in col_lower:
                    freq_str = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 'ng√†y' in col_lower and 'v·ªá sinh' in col_lower and 'g·∫ßn' in col_lower:
                    last_cleaning = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 'ch·ª©a' in col_lower and 's·∫£n ph·∫©m' in col_lower:
                    has_product = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
                elif 'tr·∫°ng th√°i' in col_lower:
                    existing_status = str(row.get(col, '')).strip() if pd.notna(row.get(col, '')) else ''
            
            # Debug first few rows
            if idx < 5:
                print(f"üîç Row {idx} extracted data:")
                print(f"  area: '{area}', device: '{device}', method: '{method}'")
                print(f"  freq_str: '{freq_str}', last_cleaning: '{last_cleaning}', has_product: '{has_product}'")
                print(f"  existing_status: '{existing_status}'")
            
            # Skip empty rows
            if not area and not device:
                if idx < 5:
                    print(f"  -> Skipping empty row {idx}")
                continue
            
            # If we have existing status and it's valid, use it
            if use_existing_status and existing_status and existing_status not in ['nan', 'None', '']:
                current_status = existing_status
                
                # Still try to calculate next plan date if we have the data
                next_plan_str = ''
                if last_cleaning and last_cleaning not in ['nan', 'None', '']:
                    if freq_str and freq_str not in ['nan', 'None', '']:
                        try:
                            freq = int(float(freq_str))
                            last_cleaning_date = parse_date(last_cleaning)
                            if last_cleaning_date:
                                next_plan_date = last_cleaning_date + timedelta(days=freq)
                                next_plan_str = next_plan_date.strftime('%d/%m/%Y')
                        except (ValueError, TypeError):
                            pass
                
                updated_values.append([area, device, method, freq_str, last_cleaning, next_plan_str, current_status, has_product])
                
                # Safe increment using .get() method
                status_counts[current_status] = status_counts.get(current_status, 0) + 1
                processed_count += 1
                
                if idx < 5:
                    print(f"  -> Using existing status: {current_status}")
                continue
                
            # Otherwise, calculate status as before
            if not last_cleaning or last_cleaning in ['nan', 'None', '']:
                status = "Ch∆∞a c√≥ d·ªØ li·ªáu"
                updated_values.append([area, device, method, freq_str, last_cleaning, "", status, has_product])
                status_counts[status] = status_counts.get(status, 0) + 1
                if idx < 5:
                    print(f"  -> Status: {status} (no cleaning date)")
                continue
    
            freq = 0
            if freq_str and freq_str not in ['nan', 'None', '']:
                try:
                    freq = int(float(freq_str))
                except ValueError:
                    freq = 0
    
            last_cleaning_date = parse_date(last_cleaning)
            if not last_cleaning_date:
                status = "ƒê·ªãnh d·∫°ng ng√†y kh√¥ng h·ª£p l·ªá"
                updated_values.append([area, device, method, freq_str, last_cleaning, "", status, has_product])
                status_counts['L·ªói'] = status_counts.get('L·ªói', 0) + 1
                if idx < 5:
                    print(f"  -> Status: {status} (invalid date: '{last_cleaning}')")
                continue
    
            next_plan_date = last_cleaning_date + timedelta(days=freq)
            next_plan_str = next_plan_date.strftime('%d/%m/%Y')
    
            days_until_next = (next_plan_date.date() - today.date()).days
            
            if days_until_next > 7:
                current_status = 'B√¨nh th∆∞·ªùng'
            elif days_until_next > 0:
                current_status = 'S·∫Øp ƒë·∫øn h·∫°n'
            elif days_until_next == 0:
                current_status = 'ƒê·∫øn h·∫°n'
            else:
                current_status = 'Qu√° h·∫°n'
    
            updated_values.append([area, device, method, freq_str, last_cleaning, next_plan_str, current_status, has_product])
            status_counts[current_status] = status_counts.get(current_status, 0) + 1
            processed_count += 1
            
            # Debug first few rows
            if idx < 5:
                print(f"  -> Calculated status: {current_status} (days until next: {days_until_next})")
                print(f"  -> Last cleaning: {last_cleaning_date.strftime('%d/%m/%Y')}, Next: {next_plan_str}")
            
            # Update the DataFrame (only if we calculated new values)
            if not use_existing_status:
                # Find the correct column names for updating
                for col in master_plan_df.columns:
                    col_lower = str(col).lower().strip()
                    if 'k·∫ø ho·∫°ch' in col_lower or ('ng√†y' in col_lower and 'ti·∫øp theo' in col_lower):
                        master_plan_df.at[idx, col] = next_plan_str
                    elif 'tr·∫°ng th√°i' in col_lower:
                        master_plan_df.at[idx, col] = current_status
            
        except Exception as e:
            print(f"‚ùå Error processing row {idx}: {str(e)}")
            print(f"‚ùå Full traceback: {traceback.format_exc()}")
            status_counts['L·ªói'] = status_counts.get('L·ªói', 0) + 1
            continue
    
    # Print processing summary
    print(f"\nüìä Processing Summary:")
    print(f"  - Total rows processed: {processed_count}")
    print(f"  - Status breakdown:")
    for status, count in status_counts.items():
        if count > 0:
            print(f"    - {status}: {count}")
    
    print(f"\nüéØ Due/Overdue Equipment Check:")
    due_count = status_counts.get('ƒê·∫øn h·∫°n', 0) + status_counts.get('Qu√° h·∫°n', 0)
    print(f"  - Equipment due for cleaning: {due_count}")
    print(f"    - ƒê·∫øn h·∫°n: {status_counts.get('ƒê·∫øn h·∫°n', 0)}")
    print(f"    - Qu√° h·∫°n: {status_counts.get('Qu√° h·∫°n', 0)}")
    
    # Update sheets data
    sheets_data['Master plan'] = master_plan_df
    
    # Update Actual Result with new cleaning records
    print("Ki·ªÉm tra v√† c·∫≠p nh·∫≠t b·∫£n ghi v·ªá sinh m·ªõi...")
    
    # Read existing records from Actual Result
    existing_records = set()  # Set of unique cleaning records (device + date)
    
    for idx, row in actual_result_df.iterrows():
        device_name = str(row.get('Thi·∫øt b·ªã', '')).strip() if pd.notna(row.get('Thi·∫øt b·ªã', '')) else ''
        cleaning_date_str = str(row.get('Ng√†y v·ªá sinh', '')).strip() if pd.notna(row.get('Ng√†y v·ªá sinh', '')) else ''
        if device_name and cleaning_date_str:
            record_key = f"{device_name}_{cleaning_date_str}"
            existing_records.add(record_key)
    
    # Identify new cleaning records from Master plan
    new_cleaning_records = []
    
    for row in updated_values:
        area, device, method, freq_str, last_cleaning, next_plan_str, status, has_product = row
        
        # Skip if no cleaning date or format is invalid
        if not last_cleaning or "kh√¥ng h·ª£p l·ªá" in status.lower() or "ch∆∞a c√≥ d·ªØ li·ªáu" in status.lower():
            continue
            
        # Create unique key for this cleaning record
        record_key = f"{device}_{last_cleaning}"
        
        # Add to Actual Result if not already recorded
        if record_key not in existing_records:
            # Default values for new records
            person = "T·ª± ƒë·ªông"  # Placeholder or default person
            result = "ƒê·∫°t"      # Default result
            notes = ""          # Empty notes
            
            # Add new cleaning record
            new_cleaning_records.append({
                'Khu v·ª±c': area,
                'Thi·∫øt b·ªã': device,
                'Ph∆∞∆°ng ph√°p': method,
                'T·∫ßn su·∫•t (ng√†y)': freq_str,
                'Ng√†y v·ªá sinh': last_cleaning,
                'Ng∆∞·ªùi th·ª±c hi·ªán': person,
                'K·∫øt qu·∫£': result,
                'Ghi ch√∫': notes
            })
            
            # Mark as processed to avoid duplicates
            existing_records.add(record_key)
    
    # Add new cleaning records to Actual Result sheet
    if new_cleaning_records:
        new_df = pd.DataFrame(new_cleaning_records)
        actual_result_df = pd.concat([actual_result_df, new_df], ignore_index=True)
        sheets_data['Actual result'] = actual_result_df
        print(f"ƒê√£ th√™m {len(new_cleaning_records)} b·∫£n ghi v·ªá sinh m·ªõi v√†o Actual Result")
    else:
        print("Kh√¥ng c√≥ b·∫£n ghi v·ªá sinh m·ªõi ƒë·ªÉ th√™m v√†o Actual Result")
    
    print(f"ƒê√£ c·∫≠p nh·∫≠t {len(updated_values)} thi·∫øt b·ªã.")
    
    # Try to upload updated file back to SharePoint
    upload_success = False
    if len(updated_values) > 0:
        print(f"\nüì§ Attempting to upload updated file...")
        try:
            upload_success = global_processor.upload_excel_file(sheets_data)
        except Exception as e:
            print(f"‚ö†Ô∏è Upload failed with error: {str(e)}")
            upload_success = False
    
    # Create local backup if upload failed but we have data
    if not upload_success and len(updated_values) > 0:
        try:
            backup_filename = f"CIP_plan_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            with pd.ExcelWriter(backup_filename, engine='openpyxl') as writer:
                for sheet_name, df in sheets_data.items():
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
            print(f"üíæ Created local backup: {backup_filename}")
        except Exception as e:
            print(f"‚ùå Failed to create local backup: {str(e)}")
    
    return updated_values

# Function to add a new cleaning record
def add_cleaning_record(area, device, method, freq, cleaning_date, person, result="ƒê·∫°t", notes=""):
    """
    Add a new cleaning record and update Master plan and Actual Result
    Note: This function would need SharePoint integration for direct updates
    """
    print(f"Adding cleaning record for {device} on {cleaning_date}")
    # Implementation would require SharePoint integration
    return "Th√†nh c√¥ng"

# Function to update cleaning result
def update_cleaning_result(device, cleaning_date, result, notes=""):
    """
    Update the result of a cleaning record in the Actual Result sheet
    Note: This function would need SharePoint integration for direct updates
    """
    print(f"Updating cleaning result for {device} on {cleaning_date}")
    # Implementation would require SharePoint integration
    return "Th√†nh c√¥ng"

# Function to update product status
def update_product_status(device, has_product):
    """
    Update the product status for a device in the Master plan
    Note: This function would need SharePoint integration for direct updates
    """
    print(f"Updating product status for {device}")
    # Implementation would require SharePoint integration
    return "Th√†nh c√¥ng"

# Function to create status chart
def create_status_chart(updated_values):
    try:
        # Create DataFrame for visualization
        df = pd.DataFrame(updated_values, columns=[
            'Khu v·ª±c', 'Thi·∫øt b·ªã', 'Ph∆∞∆°ng ph√°p', 'T·∫ßn su·∫•t (ng√†y)',
            'Ng√†y v·ªá sinh g·∫ßn nh·∫•t', 'Ng√†y k·∫ø ho·∫°ch v·ªá sinh ti·∫øp theo', 'Tr·∫°ng th√°i', 'ƒêang ch·ª©a s·∫£n ph·∫©m'
        ])
        
        # Set up figure with 2 subplots
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # First subplot: Count statuses
        status_counts = df['Tr·∫°ng th√°i'].value_counts()
        status_order = ['B√¨nh th∆∞·ªùng', 'S·∫Øp ƒë·∫øn h·∫°n', 'ƒê·∫øn h·∫°n', 'Qu√° h·∫°n']
        
        # Create a Series with all possible statuses and fill missing with 0
        status_data = pd.Series([0, 0, 0, 0], index=status_order)
        
        # Update with actual counts
        for status, count in status_counts.items():
            if status in status_data.index:
                status_data[status] = count
        
        # Create a bar chart for cleaning status
        colors = ['green', 'yellow', 'orange', 'red']
        ax1.bar(status_data.index, status_data.values, color=colors)
        ax1.set_title('Th·ªëng k√™ tr·∫°ng th√°i thi·∫øt b·ªã v·ªá sinh')
        ax1.set_ylabel('S·ªë l∆∞·ª£ng')
        ax1.grid(axis='y', linestyle='--', alpha=0.7)
        
        # Second subplot: Count product status for overdue equipment
        overdue_df = df[df['Tr·∫°ng th√°i'].isin(['ƒê·∫øn h·∫°n', 'Qu√° h·∫°n'])]
        
        # Count devices with/without product
        product_status = overdue_df['ƒêang ch·ª©a s·∫£n ph·∫©m'].fillna('Tr·ªëng').map(lambda x: 'C√≥ s·∫£n ph·∫©m' if str(x).strip() else 'Tr·ªëng')
        product_counts = product_status.value_counts()
        
        # Ensure both categories are present
        product_data = pd.Series([0, 0], index=['C√≥ s·∫£n ph·∫©m', 'Tr·ªëng'])
        for status, count in product_counts.items():
            product_data[status] = count
        
        # Create a pie chart for product status
        ax2.pie(
            product_data.values,
            labels=product_data.index,
            colors=['red', 'green'],
            autopct='%1.1f%%',
            startangle=90
        )
        ax2.set_title('Tr·∫°ng th√°i s·∫£n ph·∫©m c·ªßa thi·∫øt b·ªã c·∫ßn v·ªá sinh')
        ax2.axis('equal')
        
        plt.tight_layout()
        
        # Save chart for email
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', dpi=100)
        img_buffer.seek(0)
        
        plt.close()  # Close the plot to avoid warnings
        return img_buffer
    
    except Exception as e:
        print(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì: {str(e)}")
        return None

# Function to create results analysis chart
def create_results_chart():
    try:
        # This would need to get data from SharePoint
        # For now, return None
        return None
    
    except Exception as e:
        print(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì k·∫øt qu·∫£: {str(e)}")
        return None

# Modified send_email_report function with Outlook SMTP
def send_email_report(updated_values):
    print("ƒêang chu·∫©n b·ªã g·ª≠i email b√°o c√°o...")
    
    # Debug: Print all updated values to understand the data structure
    print(f"üîç Total updated_values: {len(updated_values)}")
    if updated_values:
        print(f"üîç Sample updated_values (first 3):")
        for i, row in enumerate(updated_values[:3]):
            print(f"  Row {i}: {row}")
            if len(row) > 6:
                print(f"    Status (index 6): '{row[6]}'")
    
    # Filter devices requiring attention
    due_rows = [row for row in updated_values if len(row) > 6 and row[6] in ['ƒê·∫øn h·∫°n', 'Qu√° h·∫°n']]
    
    print(f"üîç Filtering logic:")
    print(f"  - Looking for status in ['ƒê·∫øn h·∫°n', 'Qu√° h·∫°n']")
    print(f"  - Found {len(due_rows)} due/overdue devices")
    
    # Debug: Print status breakdown
    status_breakdown = {}
    for row in updated_values:
        if len(row) > 6:
            status = row[6]
            status_breakdown[status] = status_breakdown.get(status, 0) + 1
    
    print(f"üîç Status breakdown from updated_values:")
    for status, count in status_breakdown.items():
        print(f"  - '{status}': {count}")
    
    if due_rows:
        print(f"‚úÖ Found {len(due_rows)} devices requiring attention")
        
        # Debug: Print due devices
        print(f"üîç Due devices details:")
        for i, row in enumerate(due_rows[:5]):  # Show first 5
            print(f"  {i+1}. {row[0]} - {row[1]} - Status: {row[6]}")
        
        try:
            # Create charts
            status_img_buffer = create_status_chart(updated_values)
            results_img_buffer = create_results_chart()
            
            # Split the devices by area
            ro_station_rows = [row for row in due_rows if 'tr·∫°m ro' in str(row[0]).lower()]
            other_area_rows = [row for row in due_rows if 'tr·∫°m ro' not in str(row[0]).lower()]
            
            print(f"üîç Area breakdown:")
            print(f"  - RO station devices: {len(ro_station_rows)}")
            print(f"  - Other area devices: {len(other_area_rows)}")
            
            # Define email recipient lists
            ro_recipients = [
                "mmb-ktcncsd@msc.masangroup.com", 
                "mmb-baotri-utilities@msc.masangroup.com", 
            ]
            
            other_recipients = [
                "mmb-ktcncsd@msc.masangroup.com",
            ]
            
            # Send RO station email if there are relevant items
            if ro_station_rows:
                print(f"üìß Sending email for RO station ({len(ro_station_rows)} devices)")
                send_area_specific_email(
                    ro_station_rows, 
                    ro_recipients, 
                    "Tr·∫°m RO", 
                    status_img_buffer, 
                    results_img_buffer
                )
            
            # Send other areas email if there are relevant items
            if other_area_rows:
                print(f"üìß Sending email for other areas ({len(other_area_rows)} devices)")
                send_area_specific_email(
                    other_area_rows, 
                    other_recipients, 
                    "Khu v·ª±c mu·ªëi, c·ªët, ch·∫ø bi·∫øn m·∫Øm", 
                    status_img_buffer, 
                    results_img_buffer
                )
                
            print("‚úÖ Email ƒë√£ ƒë∆∞·ª£c g·ª≠i k√®m b·∫£ng HTML v√† bi·ªÉu ƒë·ªì.")
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ª≠i email: {str(e)}")
            print(f"‚ùå Traceback: {traceback.format_exc()}")
            return False
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ thi·∫øt b·ªã ƒë·∫øn h·∫°n/qu√° h·∫°n, kh√¥ng g·ª≠i email.")
        print("üîç This might be due to:")
        print("  1. Date parsing issues")
        print("  2. Incorrect status calculation")
        print("  3. Data structure problems")
        print("  4. Column mapping issues")
        return True

# Helper function to send area-specific emails with Graph API
def send_area_specific_email(filtered_rows, recipients, area_name, status_img_buffer, results_img_buffer):
    """
    Send an email for a specific area with the filtered rows using Microsoft Graph API
    """
    global global_processor  # Use the global processor
    
    try:
        if not global_processor or not global_processor.access_token:
            print("‚ùå No valid access token for Graph API")
            return False
            
        print(f"üìß Preparing email via Microsoft Graph API for {area_name}")
        
        # Prepare data for email summary
        empty_tanks = [row for row in filtered_rows if not str(row[7]).strip()]
        filled_tanks = [row for row in filtered_rows if str(row[7]).strip()]
        
        # Create HTML content
        html_content = f"""
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; }}
                table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; color: #333; }}
                .overdue {{ background-color: #ffcccc; }}
                .due-today {{ background-color: #ffeb99; }}
                .due-soon {{ background-color: #e6ffcc; }}
                .has-product {{ color: #cc0000; font-weight: bold; }}
                .empty {{ color: #009900; }}
                h2 {{ color: #003366; }}
                h3 {{ color: #004d99; margin-top: 25px; }}
                .summary {{ margin: 20px 0; }}
                .footer {{ margin-top: 30px; font-size: 0.9em; color: #666; }}
            </style>
        </head>
        <body>
            <h2>B√°o c√°o v·ªá sinh thi·∫øt b·ªã - {area_name} - {datetime.today().strftime("%d/%m/%Y")}</h2>
            
            <div class="summary">
                <p><strong>T·ªïng s·ªë thi·∫øt b·ªã c·∫ßn v·ªá sinh:</strong> {len(filtered_rows)}</p>
                <p><strong>Thi·∫øt b·ªã tr·ªëng c√≥ th·ªÉ v·ªá sinh ngay:</strong> {len(empty_tanks)}</p>
                <p><strong>Thi·∫øt b·ªã ƒëang ch·ª©a s·∫£n ph·∫©m c·∫ßn l√™n k·∫ø ho·∫°ch:</strong> {len(filled_tanks)}</p>
            </div>
            
            <h3>Danh s√°ch thi·∫øt b·ªã c·∫ßn v·ªá sinh:</h3>
            <table>
                <thead>
                    <tr>
                        <th>Khu v·ª±c</th>
                        <th>Thi·∫øt b·ªã</th>
                        <th>Ph∆∞∆°ng ph√°p</th>
                        <th>T·∫ßn su·∫•t (ng√†y)</th>
                        <th>Ng√†y v·ªá sinh g·∫ßn nh·∫•t (KQ)</th>
                        <th>Ng√†y k·∫ø ho·∫°ch v·ªá sinh ti·∫øp theo (KH)</th>
                        <th>Tr·∫°ng th√°i</th>
                        <th>ƒêang ch·ª©a s·∫£n ph·∫©m</th>
                    </tr>
                </thead>
                <tbody>
        """
        
        # Add all tanks to the table (both empty and with product)
        # Sort the rows to prioritize empty tanks first
        sorted_rows = sorted(filtered_rows, key=lambda row: 1 if str(row[7]).strip() else 0)
        
        for row in sorted_rows:
            area, device, method, freq_str, last_cleaning, next_plan_str, status, has_product = row
            
            # Define CSS class based on status
            css_class = ""
            if status == "Qu√° h·∫°n":
                css_class = "overdue"
            elif status == "ƒê·∫øn h·∫°n":
                css_class = "due-today"
            
            # Define product status class
            product_class = "has-product" if str(has_product).strip() else "empty"
            
            html_content += f"""
                    <tr class="{css_class}">
                        <td>{area}</td>
                        <td>{device}</td>
                        <td>{method}</td>
                        <td>{freq_str}</td>
                        <td>{last_cleaning}</td>
                        <td>{next_plan_str}</td>
                        <td>{status}</td>
                        <td class="{product_class}">{has_product}</td>
                    </tr>
            """
        
        html_content += """
                </tbody>
            </table>
            
            <div class="footer">
                <p>Vui l√≤ng xem SharePoint ƒë·ªÉ bi·∫øt chi ti·∫øt v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i c·ªßa c√°c thi·∫øt b·ªã.</p>
                <p>Email n√†y ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫°o b·ªüi h·ªá th·ªëng. Vui l√≤ng kh√¥ng tr·∫£ l·ªùi.</p>
            </div>
        </body>
        </html>
        """
        
        # Prepare email data for Graph API
        email_data = {
            "message": {
                "subject": f"B√°o c√°o v·ªá sinh thi·∫øt b·ªã - {area_name} - {datetime.today().strftime('%d/%m/%Y')}",
                "body": {
                    "contentType": "HTML",
                    "content": html_content
                },
                "toRecipients": []
            }
        }
        
        # Add recipients
        for recipient in recipients:
            email_data["message"]["toRecipients"].append({
                "emailAddress": {
                    "address": recipient
                }
            })
        
        # Prepare attachments if available
        attachments = []
        
        if status_img_buffer:
            status_img_buffer.seek(0)
            status_img_data = status_img_buffer.read()
            status_img_b64 = base64.b64encode(status_img_data).decode('utf-8')
            
            attachments.append({
                "@odata.type": "#microsoft.graph.fileAttachment",
                "name": "cleaning_status.png",
                "contentType": "image/png",
                "contentBytes": status_img_b64
            })
        
        if results_img_buffer:
            results_img_buffer.seek(0)
            results_img_data = results_img_buffer.read()
            results_img_b64 = base64.b64encode(results_img_data).decode('utf-8')
            
            attachments.append({
                "@odata.type": "#microsoft.graph.fileAttachment", 
                "name": "cleaning_results.png",
                "contentType": "image/png",
                "contentBytes": results_img_b64
            })
        
        if attachments:
            email_data["message"]["attachments"] = attachments
        
        # Send email via Graph API
        graph_url = "https://graph.microsoft.com/v1.0/me/sendMail"
        headers = {
            'Authorization': f'Bearer {global_processor.access_token}',
            'Content-Type': 'application/json'
        }
        
        print(f"üì§ Sending email via Graph API to {len(recipients)} recipients...")
        print(f"üîó Graph URL: {graph_url}")
        
        response = requests.post(graph_url, headers=headers, json=email_data, timeout=60)
        
        if response.status_code == 202:
            print("‚úÖ Email sent successfully via Graph API")
            print(f"‚úÖ Email cho {area_name} ƒë√£ ƒë∆∞·ª£c g·ª≠i ƒë·∫øn {len(recipients)} ng∆∞·ªùi nh·∫≠n.")
            return True
        elif response.status_code == 401:
            print("‚ùå Graph API Authentication Error - Token may have expired")
            print("üîÑ Attempting to refresh token...")
            if global_processor.refresh_access_token():
                print("‚úÖ Token refreshed, retrying email send...")
                headers['Authorization'] = f'Bearer {global_processor.access_token}'
                response = requests.post(graph_url, headers=headers, json=email_data, timeout=60)
                if response.status_code == 202:
                    print("‚úÖ Email sent successfully after token refresh")
                    return True
            print("‚ùå Failed to send email even after token refresh")
            return False
        elif response.status_code == 403:
            print("‚ùå Graph API Permission Error")
            print("üí° Please ensure Mail.Send permission is granted in Azure App Registration:")
            print("   1. Go to Azure Portal ‚Üí App registrations")
            print("   2. Find your app ‚Üí API permissions")
            print("   3. Add Microsoft Graph ‚Üí Delegated permissions ‚Üí Mail.Send")
            print("   4. Grant admin consent")
            return False
        else:
            print(f"‚ùå Graph API Error: {response.status_code}")
            print(f"‚ùå Response: {response.text[:500]}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error sending email via Graph API: {str(e)}")
        print(f"‚ùå Traceback: {traceback.format_exc()}")
        return False

# Main function to run everything
def run_update():
    print("B·∫Øt ƒë·∫ßu c·∫≠p nh·∫≠t h·ªá th·ªëng v·ªá sinh thi·∫øt b·ªã t·ª´ SharePoint...")
    
    try:
        # Update cleaning schedule and get updated values
        updated_values = update_cleaning_schedule()
        
        # Send email report
        send_email_report(updated_values)
        
        print("Ho√†n th√†nh c·∫≠p nh·∫≠t.")
        return True
    except Exception as e:
        print(f"L·ªói: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return False

# Run the update if executed directly
if __name__ == "__main__":
    success = run_update()
    if success:
        print("‚úÖ CIP Cleaning automation completed successfully!")
    else:
        print("‚ùå CIP Cleaning automation failed!")
        sys.exit(1)
